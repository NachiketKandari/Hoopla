[
  {
    "type": "function",
    "name": "copy_py_files_to_txt",
    "filepath": "copy_code.py",
    "filename": "copy_code.py",
    "content": "def copy_py_files_to_txt(source_dir: pathlib.Path, target_dir: pathlib.Path):\n    \n    try:\n        target_dir.mkdir(parents=True, exist_ok=True)\n    except OSError as e:\n        print(f\"Error creating directory {target_dir}: {e}\", file=sys.stderr)\n        return\n\n    print(f\"\\n--- Processing files in: {source_dir} ---\")\n    \n    py_files = list(source_dir.glob(\"*.py\"))\n    \n    if not py_files:\n        print(\"  No .py files found.\")\n        return\n\n    for py_file_path in py_files:\n        try:\n            target_file_name = py_file_path.stem + \".txt\"\n            target_file_path = target_dir / target_file_name\n            \n            content = py_file_path.read_text(encoding=\"utf-8\")\n            \n            target_file_path.write_text(content, encoding=\"utf-8\")\n            \n            print(f\"  Copied: {py_file_path.name}  ->  {target_file_path.name}\")\n            \n        except Exception as e:\n            print(f\"  Error processing {py_file_path.name}: {e}\", file=sys.stderr)\n\n    print(f\"--- Finished processing {len(py_files)} files. ---\")",
    "start_line": 4,
    "end_line": 34,
    "description": "The `copy_py_files_to_txt` function copies the contents of all Python (.py) files within a specified `source_dir` to a `target_dir`. It iterates through `.py` files, reads their content using UTF-8 encoding, and writes the content to new `.txt` files with the same name (e.g., `my_script.py` becomes `my_script.txt`). The function creates the `target_dir` if it doesn't exist and handles potential file I/O errors.  It prints progress messages to the console, including errors, and reports the number of files processed."
  },
  {
    "type": "function",
    "name": "main",
    "filepath": "copy_code.py",
    "filename": "copy_code.py",
    "content": "def main():\n    base_dir = pathlib.Path.cwd()\n    \n    cli_source = base_dir / \"cli\"\n    lib_source = base_dir / \"cli\" / \"lib\"\n    app_source = base_dir / \"app\"\n    \n    cli_target = base_dir / \"clicpy\"\n    lib_target = base_dir / \"libcpy\"\n    app_target = base_dir / \"appcpy\"\n    \n    print(\"Starting copy process...\")\n    \n    copy_py_files_to_txt(cli_source, cli_target)\n    copy_py_files_to_txt(lib_source, lib_target)\n    copy_py_files_to_txt(app_source, app_target)\n    \n    print(\"\\nAll tasks complete.\")",
    "start_line": 36,
    "end_line": 53,
    "description": "The `main` function copies Python files from specified source directories to target directories, converting each `.py` file to a `.txt` file.  It uses `pathlib` to define source and target paths for the `cli`, `lib`, and `app` components.  It calls `copy_py_files_to_txt` (assumed to exist) to perform the file conversion. The function starts by printing a \"Starting copy process...\" message and ends with an \"All tasks complete.\" message."
  },
  {
    "type": "function",
    "name": "hash_password",
    "filepath": "app/auth.py",
    "filename": "auth.py",
    "content": "def hash_password(password: str) -> str:\n    \"\"\"Hash a password using bcrypt.\"\"\"\n    salt = bcrypt.gensalt()\n    return bcrypt.hashpw(password.encode('utf-8'), salt).decode('utf-8')",
    "start_line": 13,
    "end_line": 16,
    "description": "The `hash_password` function securely hashes a given password using the bcrypt algorithm. It takes a plain text `password` (string) as input. Inside the function, a random salt is generated using `bcrypt.gensalt()`. The password, encoded to UTF-8, is then hashed along with the salt using `bcrypt.hashpw()`. The resulting hash, encoded back to a UTF-8 string, is returned. This function ensures robust password storage by utilizing a strong hashing algorithm and a unique salt for each password."
  },
  {
    "type": "function",
    "name": "verify_password",
    "filepath": "app/auth.py",
    "filename": "auth.py",
    "content": "def verify_password(password: str, password_hash: str) -> bool:\n    \"\"\"Verify a password against a hash.\"\"\"\n    try:\n        return bcrypt.checkpw(password.encode('utf-8'), password_hash.encode('utf-8'))\n    except (ValueError, AttributeError) as e:\n        # Invalid hash format (e.g., SHA256 instead of bcrypt)\n        return False",
    "start_line": 19,
    "end_line": 25,
    "description": "The `verify_password` function checks if a given password matches a stored password hash. It takes the user-provided `password` (string) and the stored `password_hash` (string) as input. It utilizes the `bcrypt.checkpw` function to perform the comparison.  The function encodes both the password and hash to UTF-8 before comparison.  It returns `True` if the password matches the hash, and `False` otherwise.  Importantly, the function also handles potential errors, such as invalid hash formats, returning `False` if an exception occurs during the verification process."
  },
  {
    "type": "function",
    "name": "register_user",
    "filepath": "app/auth.py",
    "filename": "auth.py",
    "content": "def register_user(username: str, password: str) -> Tuple[bool, str]:\n    \"\"\"\n    Register a new user.\n    Returns (success, message)\n    \"\"\"\n    if not username or not password:\n        return False, \"Username and password are required\"\n    \n    if len(username) < 3:\n        return False, \"Username must be at least 3 characters\"\n    \n    if len(password) < 6:\n        return False, \"Password must be at least 6 characters\"\n    \n    password_hash = hash_password(password)\n    success = create_user(username, password_hash)\n    \n    if success:\n        return True, \"Registration successful! Please login.\"\n    else:\n        return False, \"Username already exists\"",
    "start_line": 28,
    "end_line": 48,
    "description": "The `register_user` function handles user registration. It takes a `username` (string) and `password` (string) as input. It first validates the inputs, ensuring they meet length requirements. It then hashes the password using `hash_password` and attempts to create the user in a database using `create_user`. The function returns a tuple: `(success, message)`. Success is a boolean indicating registration success, and message provides feedback, such as error details (e.g., username already exists) or confirmation."
  },
  {
    "type": "function",
    "name": "authenticate_user",
    "filepath": "app/auth.py",
    "filename": "auth.py",
    "content": "def authenticate_user(username: str, password: str) -> Tuple[Optional[int], str]:\n    \"\"\"\n    Authenticate a user.\n    Returns (user_id, message). user_id is None if authentication failed.\n    \"\"\"\n    if not username or not password:\n        return None, \"Username and password are required\"\n    \n    user = get_user_by_username(username)\n    if not user:\n        return None, \"Invalid username or password\"\n    \n    if not verify_password(password, user['password_hash']):\n        return None, \"Invalid username or password\"\n    \n    return user['id'], \"Login successful\"",
    "start_line": 51,
    "end_line": 66,
    "description": "The `authenticate_user` function verifies user credentials. It takes a `username` and `password` as input.  It first checks for empty input.  Then, it retrieves user data using `get_user_by_username`. If the user exists and the provided password, when verified against the stored hash via `verify_password`, matches the stored hash, the function returns a tuple containing the user's `id` and a success message. Otherwise, it returns `None` and an error message."
  },
  {
    "type": "function",
    "name": "check_rate_limit",
    "filepath": "app/auth.py",
    "filename": "auth.py",
    "content": "def check_rate_limit(user_id: int, is_system_api: bool) -> Tuple[bool, int]:\n    \"\"\"\n    Check if user can make a request.\n    Returns (allowed, requests_left)\n    - SYSTEM API (Gemini): 50 requests/day limit (unlimited for admin)\n    - User API (Ollama): No limit\n    \"\"\"\n    # Check if user is admin - unlimited access\n    user = get_user_by_id(user_id)\n    if user and user.get('is_admin', 0) == 1:\n        return True, 999999  # Unlimited for admin\n    \n    if not is_system_api:\n        # User API has no limit\n        return True, -1  # -1 indicates unlimited\n    \n    requests_left = get_user_requests_left(user_id)\n    \n    if requests_left > 0:\n        return True, requests_left\n    else:\n        return False, 0",
    "start_line": 69,
    "end_line": 90,
    "description": "The `check_rate_limit` function determines if a user can make an API request, considering rate limits. It takes a `user_id` and a boolean `is_system_api` as input. For system APIs (e.g., Gemini), it enforces a 50 requests/day limit unless the user is an admin.  User API requests (e.g., Ollama) have no limit.  The function uses `get_user_by_id` to check admin status and `get_user_requests_left` to check remaining requests. It returns a tuple: `(allowed: bool, requests_left: int)`, where `requests_left` indicates remaining requests or -1 for unlimited access."
  },
  {
    "type": "function",
    "name": "consume_rate_limit",
    "filepath": "app/auth.py",
    "filename": "auth.py",
    "content": "def consume_rate_limit(user_id: int, is_system_api: bool) -> bool:\n    \"\"\"\n    Consume a rate limit for a user.\n    Returns True if successful, False if limit reached.\n    \"\"\"\n    if not is_system_api:\n        # User API has no limit\n        return True\n    \n    return decrement_user_requests(user_id)",
    "start_line": 93,
    "end_line": 102,
    "description": "The `consume_rate_limit` function manages rate limiting for user requests. It checks if the request originates from a system API, bypassing rate limits if so. For regular user APIs (is_system_api=False), the function immediately returns True, granting access. For system APIs, it calls `decrement_user_requests` with the `user_id`, which presumably attempts to decrement the user's remaining request count. It returns True if the decrement was successful (limit not reached), and False otherwise."
  },
  {
    "type": "function",
    "name": "get_current_user_id",
    "filepath": "app/auth.py",
    "filename": "auth.py",
    "content": "def get_current_user_id() -> Optional[int]:\n    \"\"\"Get the current logged-in user ID from session state.\"\"\"\n    return st.session_state.get('user_id')",
    "start_line": 105,
    "end_line": 107,
    "description": "The `get_current_user_id` function retrieves the ID of the currently logged-in user. It accesses the `st.session_state` object, which stores application-specific data within a Streamlit session. The function uses the `.get()` method to safely retrieve the value associated with the key 'user_id'. This key is expected to hold an integer representing the user's ID. It returns this user ID as an optional integer, or `None` if the 'user_id' key is not present in the session state."
  },
  {
    "type": "function",
    "name": "is_user_logged_in",
    "filepath": "app/auth.py",
    "filename": "auth.py",
    "content": "def is_user_logged_in() -> bool:\n    \"\"\"Check if a user is logged in.\"\"\"\n    return get_current_user_id() is not None",
    "start_line": 110,
    "end_line": 112,
    "description": "The `is_user_logged_in` function determines if a user is currently authenticated. It leverages an internal function, `get_current_user_id()`, which likely retrieves the ID of the logged-in user. The function returns a boolean value: `True` if `get_current_user_id()` returns a non-`None` value (indicating a user ID is present), implying a user is logged in, and `False` otherwise. Essentially, it checks for the existence of a valid user session."
  },
  {
    "type": "function",
    "name": "login_user",
    "filepath": "app/auth.py",
    "filename": "auth.py",
    "content": "def login_user(user_id: int):\n    \"\"\"Set user as logged in.\"\"\"\n    st.session_state.user_id = user_id\n    user = get_user_by_id(user_id)\n    if user:\n        st.session_state.username = user['username']",
    "start_line": 115,
    "end_line": 120,
    "description": "The `login_user` function simulates a user login within a Streamlit application. It takes an integer `user_id` as input, assigning it to the current session's `user_id`. It then retrieves the user's data using `get_user_by_id`. If the user exists, it sets the `username` from the retrieved user data within the session state. This function effectively tracks and stores the logged-in user's identifier and username across subsequent interactions within the Streamlit application."
  },
  {
    "type": "function",
    "name": "logout_user",
    "filepath": "app/auth.py",
    "filename": "auth.py",
    "content": "def logout_user():\n    \"\"\"Logout the current user.\"\"\"\n    if 'user_id' in st.session_state:\n        del st.session_state.user_id\n    if 'username' in st.session_state:\n        del st.session_state.username",
    "start_line": 123,
    "end_line": 128,
    "description": "The `logout_user` function clears a user's session data within a Streamlit application. It removes the `user_id` and `username` key-value pairs from the `st.session_state` dictionary if they exist. This action effectively logs the user out by invalidating their session and removing associated authentication information. The function doesn't take any parameters and implicitly updates the application's session state."
  },
  {
    "type": "function",
    "name": "get_db_connection",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def get_db_connection():\n    \"\"\"Get a database connection.\"\"\"\n    # Ensure data directory exists\n    DB_PATH.parent.mkdir(parents=True, exist_ok=True)\n    \n    conn = sqlite3.connect(str(DB_PATH))\n    conn.row_factory = sqlite3.Row\n    return conn",
    "start_line": 18,
    "end_line": 25,
    "description": "The `get_db_connection` function establishes a connection to an SQLite database. It first creates the necessary directory for the database file if it doesn't already exist, ensuring data persistence. Then, it utilizes the `sqlite3.connect()` function to establish the database connection using a path defined by the global variable `DB_PATH`. Crucially, it configures the connection to use `sqlite3.Row` for result access, enabling dictionary-like access to query results. The function returns the established database connection object."
  },
  {
    "type": "function",
    "name": "init_database",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def init_database():\n    \"\"\"Initialize the database with required tables.\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    # Users table\n    cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS users (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            username TEXT UNIQUE NOT NULL,\n            password_hash TEXT NOT NULL,\n            requests_left INTEGER DEFAULT 50,\n            last_reset_date DATE DEFAULT CURRENT_DATE,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\n    \"\"\")\n    \n    # Chat history table\n    cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS chat_history (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            user_id INTEGER NOT NULL,\n            query_type TEXT NOT NULL,\n            query_text TEXT,\n            query_image_base64 TEXT,\n            response_text TEXT,\n            response_results TEXT,\n            model_type TEXT,\n            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            FOREIGN KEY (user_id) REFERENCES users(id)\n        )\n    \"\"\")\n    \n    # Conversation history table (simplified for chat and RAG)\n    cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS conversation_history (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            user_id INTEGER NOT NULL,\n            mode TEXT NOT NULL CHECK(mode IN ('chat', 'rag', 'rag_summarize', 'rag_citations', 'rag_question')),\n            query TEXT NOT NULL,\n            response TEXT NOT NULL,\n            model_type TEXT,\n            deleted BOOLEAN DEFAULT 0,\n            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            FOREIGN KEY (user_id) REFERENCES users(id)\n        )\n    \"\"\")\n    \n    # Add deleted column to existing table if it doesn't exist\n    try:\n        cursor.execute(\"ALTER TABLE conversation_history ADD COLUMN deleted BOOLEAN DEFAULT 0\")\n    except sqlite3.OperationalError:\n        pass  # Column already exists\n    \n    # Add is_admin column to users table if it doesn't exist\n    try:\n        cursor.execute(\"ALTER TABLE users ADD COLUMN is_admin BOOLEAN DEFAULT 0\")\n    except sqlite3.OperationalError:\n        pass  # Column already exists\n    \n    # Admin chat sessions table\n    cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS admin_chat_sessions (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            session_name TEXT NOT NULL,\n            model_name TEXT NOT NULL,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\n    \"\"\")\n    \n    # Admin chat messages table\n    cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS admin_chat_messages (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            session_id INTEGER NOT NULL,\n            role TEXT NOT NULL CHECK(role IN ('user', 'assistant')),\n            content TEXT NOT NULL,\n            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            FOREIGN KEY (session_id) REFERENCES admin_chat_sessions(id)\n        )\n    \"\"\")\n    \n    conn.commit()\n    conn.close()",
    "start_line": 28,
    "end_line": 112,
    "description": "The `init_database` function initializes a SQLite database with several tables essential for user management, chat history, and administrative functions. It utilizes `CREATE TABLE IF NOT EXISTS` statements to define tables for users, chat logs (with query and response details), conversation history (for chat and RAG modes), admin chat sessions, and admin chat messages.  It also adds a `deleted` column to the `conversation_history` table and an `is_admin` column to the `users` table. Finally, it commits the changes and closes the database connection."
  },
  {
    "type": "function",
    "name": "reset_daily_requests",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def reset_daily_requests():\n    \"\"\"Reset daily requests for all users if it's a new day.\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    today = date.today()\n    cursor.execute(\"\"\"\n        UPDATE users \n        SET requests_left = 50, last_reset_date = ?\n        WHERE last_reset_date < ?\n    \"\"\", (today, today))\n    \n    conn.commit()\n    conn.close()",
    "start_line": 155,
    "end_line": 168,
    "description": "The `reset_daily_requests` function resets the daily request allowance for all users. It checks if the current day is different from the last reset date stored in the user database. If it's a new day, it updates the `requests_left` to 50 for all users whose `last_reset_date` is earlier than the current date.  The function uses a database connection to perform an update query on the `users` table and commits the changes before closing the connection."
  },
  {
    "type": "function",
    "name": "create_user",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def create_user(username: str, password_hash: str) -> bool:\n    \"\"\"Create a new user. Returns True if successful, False if username exists.\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    try:\n        cursor.execute(\"\"\"\n            INSERT INTO users (username, password_hash, requests_left, last_reset_date)\n            VALUES (?, ?, 50, ?)\n        \"\"\", (username, password_hash, date.today()))\n        conn.commit()\n        return True\n    except sqlite3.IntegrityError:\n        return False\n    finally:\n        conn.close()",
    "start_line": 171,
    "end_line": 186,
    "description": "The `create_user` function adds a new user to a database. It takes a `username` (string) and a pre-computed `password_hash` (string) as input.  It attempts to insert these values, along with default values for `requests_left` and `last_reset_date`, into a 'users' table. It uses an `INSERT` SQL query and utilizes parameterized queries for security.  It returns `True` if the insertion is successful and `False` if a `sqlite3.IntegrityError` occurs, indicating the username already exists. The database connection is properly closed in a `finally` block to ensure resource cleanup."
  },
  {
    "type": "function",
    "name": "get_user_by_username",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def get_user_by_username(username: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Get user by username. Returns None if not found.\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute(\"SELECT * FROM users WHERE username = ?\", (username,))\n    row = cursor.fetchone()\n    conn.close()\n    \n    if row:\n        return dict(row)\n    return None",
    "start_line": 189,
    "end_line": 200,
    "description": "The `get_user_by_username` function retrieves a user's information from a database based on their username. It takes a `username` string as input. It establishes a database connection, executes a SQL query to select the user's data from the `users` table where the username matches the input. If a matching user is found, the function returns a dictionary representation of the user's data; otherwise, it returns `None`.  The database connection is closed after the query."
  },
  {
    "type": "function",
    "name": "get_user_by_id",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def get_user_by_id(user_id: int) -> Optional[Dict[str, Any]]:\n    \"\"\"Get user by ID. Returns None if not found.\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute(\"SELECT * FROM users WHERE id = ?\", (user_id,))\n    row = cursor.fetchone()\n    conn.close()\n    \n    if row:\n        return dict(row)\n    return None",
    "start_line": 203,
    "end_line": 214,
    "description": "The `get_user_by_id` function retrieves a user's information from a database based on their unique ID. It takes an integer `user_id` as input.  It connects to a database, executes a SQL query to select the user with the matching ID, and fetches the result. If a user is found (i.e., a row is returned), the function transforms the row into a dictionary and returns it; otherwise, it returns `None`.  This function utilizes database interaction and row processing to provide user data lookup functionality."
  },
  {
    "type": "function",
    "name": "decrement_user_requests",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def decrement_user_requests(user_id: int) -> bool:\n    \"\"\"Decrement user's request count. Returns True if successful, False if limit reached.\"\"\"\n    # Don't reset here - it's called by get_user_requests_left which is always called before this\n    # reset_daily_requests()  # Removed to avoid redundant DB operations\n    \n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute(\"\"\"\n        UPDATE users \n        SET requests_left = requests_left - 1\n        WHERE id = ? AND requests_left > 0\n    \"\"\", (user_id,))\n    \n    success = cursor.rowcount > 0\n    conn.commit()\n    conn.close()\n    \n    return success",
    "start_line": 217,
    "end_line": 235,
    "description": "This Python function, `decrement_user_requests`, reduces a user's remaining request count in a database. It takes a `user_id` (integer) as input.  It attempts to decrement `requests_left` for the specified user within the database, only if the current `requests_left` is greater than zero.  It uses a SQL `UPDATE` statement.  The function returns `True` if the decrement was successful (a row was updated), and `False` otherwise (request limit reached or user not found). Database connections are handled within the function."
  },
  {
    "type": "function",
    "name": "get_user_requests_left",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def get_user_requests_left(user_id: int) -> int:\n    \"\"\"Get remaining requests for a user.\"\"\"\n    reset_daily_requests()  # Reset if needed\n    \n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute(\"SELECT requests_left FROM users WHERE id = ?\", (user_id,))\n    row = cursor.fetchone()\n    conn.close()\n    \n    if row:\n        return row['requests_left']\n    return 0",
    "start_line": 238,
    "end_line": 251,
    "description": "The `get_user_requests_left` function retrieves the number of API requests remaining for a given user. It takes a user ID as an integer input.  The function first resets daily request counters using `reset_daily_requests()`. It then queries a database, fetching the `requests_left` value from the 'users' table based on the provided `user_id`.  The function returns the integer value of `requests_left`, or 0 if the user is not found in the database.  Database connection management is handled internally."
  },
  {
    "type": "function",
    "name": "add_chat_history",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def add_chat_history(\n    user_id: int,\n    query_type: str,\n    query_text: Optional[str] = None,\n    query_image_base64: Optional[str] = None,\n    response_text: Optional[str] = None,\n    response_results: Optional[str] = None,\n    model_type: Optional[str] = None\n):\n    \"\"\"Add a chat history entry.\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute(\"\"\"\n        INSERT INTO chat_history \n        (user_id, query_type, query_text, query_image_base64, response_text, response_results, model_type)\n        VALUES (?, ?, ?, ?, ?, ?, ?)\n    \"\"\", (user_id, query_type, query_text, query_image_base64, response_text, response_results, model_type))\n    \n    conn.commit()\n    conn.close()",
    "start_line": 254,
    "end_line": 274,
    "description": "The `add_chat_history` function stores a user's chat interaction in a database. It takes parameters like `user_id`, `query_type`, `query_text`, `query_image_base64`, `response_text`, `response_results`, and `model_type` to record the conversation details.  It establishes a database connection, constructs an SQL `INSERT` statement, and executes it, saving the information to the `chat_history` table. The function then commits the changes and closes the database connection.  There is no return value."
  },
  {
    "type": "function",
    "name": "get_chat_history",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def get_chat_history(user_id: int, limit: int = 50) -> List[Dict[str, Any]]:\n    \"\"\"Get chat history for a user.\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute(\"\"\"\n        SELECT * FROM chat_history \n        WHERE user_id = ? \n        ORDER BY timestamp DESC \n        LIMIT ?\n    \"\"\", (user_id, limit))\n    \n    rows = cursor.fetchall()\n    conn.close()\n    \n    return [dict(row) for row in rows]",
    "start_line": 277,
    "end_line": 292,
    "description": "The `get_chat_history` function retrieves a user's chat history from a database. It takes a `user_id` (integer) as input, specifying which user's history to fetch, and an optional `limit` (integer, defaults to 50) to restrict the number of returned messages.  It queries the `chat_history` table, filtering by `user_id` and ordering messages by `timestamp` in descending order.  The function returns a list of dictionaries, where each dictionary represents a chat message retrieved from the database."
  },
  {
    "type": "function",
    "name": "add_conversation",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def add_conversation(\n    user_id: int,\n    mode: str,\n    query: str,\n    response: str,\n    model_type: Optional[str] = None\n):\n    \"\"\"Add a conversation entry (chat or RAG).\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute(\"\"\"\n        INSERT INTO conversation_history \n        (user_id, mode, query, response, model_type)\n        VALUES (?, ?, ?, ?, ?)\n    \"\"\", (user_id, mode, query, response, model_type))\n    \n    conn.commit()\n    conn.close()",
    "start_line": 295,
    "end_line": 313,
    "description": "The `add_conversation` function stores a user's chat interaction into a database. It takes the user's ID, interaction mode (e.g., \"chat\" or \"RAG\"), user query, the system's response, and an optional model type as input. The function establishes a connection to a database, inserts the provided information into the `conversation_history` table, and commits the changes. Finally, it closes the database connection. This process effectively logs each conversational turn."
  },
  {
    "type": "function",
    "name": "get_conversation_history",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def get_conversation_history(user_id: int, mode: Optional[str] = None, limit: int = 50) -> List[Dict[str, Any]]:\n    \"\"\"Get conversation history for a user. Optionally filter by mode (chat/rag).\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    if mode:\n        cursor.execute(\"\"\"\n            SELECT * FROM conversation_history \n            WHERE user_id = ? AND mode = ?\n            ORDER BY timestamp DESC \n            LIMIT ?\n        \"\"\", (user_id, mode, limit))\n    else:\n        cursor.execute(\"\"\"\n            SELECT * FROM conversation_history \n            WHERE user_id = ? \n            ORDER BY timestamp DESC \n            LIMIT ?\n        \"\"\", (user_id, limit))\n    \n    rows = cursor.fetchall()\n    conn.close()\n    \n    return [dict(row) for row in rows]",
    "start_line": 316,
    "end_line": 339,
    "description": "The `get_conversation_history` function retrieves a user's conversation history from a database. It takes the `user_id` as a required parameter and optionally filters by `mode` (e.g., \"chat\", \"rag\") and a `limit` on the number of returned messages.  The function queries the `conversation_history` table using SQL, ordering results by timestamp.  It returns a list of dictionaries, where each dictionary represents a conversation entry. The function uses a database connection and cursor to execute the SQL query and fetch the results."
  },
  {
    "type": "function",
    "name": "get_recent_chat_messages",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def get_recent_chat_messages(user_id: int, limit: int = 10) -> List[Dict[str, Any]]:\n    \"\"\"Get recent chat messages for loading chat history (excludes deleted messages).\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute(\"\"\"\n        SELECT query, response, timestamp FROM conversation_history \n        WHERE user_id = ? AND mode = 'chat' AND deleted = 0\n        ORDER BY timestamp ASC \n        LIMIT ?\n    \"\"\", (user_id, limit))\n    \n    rows = cursor.fetchall()\n    conn.close()\n    \n    return [dict(row) for row in rows]",
    "start_line": 342,
    "end_line": 357,
    "description": "The `get_recent_chat_messages` function retrieves a user's recent chat history from a database. It takes a `user_id` (integer) and an optional `limit` (integer, default 10) specifying the number of messages to retrieve. It queries the `conversation_history` table, filtering for messages with the given `user_id`, a `mode` of 'chat', and where `deleted` is 0 (not deleted).  Results are ordered by timestamp (ascending). The function returns a list of dictionaries, where each dictionary represents a chat message, containing the query, response, and timestamp."
  },
  {
    "type": "function",
    "name": "mark_conversations_as_deleted",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def mark_conversations_as_deleted(user_id: int, mode: str = 'chat'):\n    \"\"\"Soft delete conversations for a user by mode (chat/rag/all).\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    if mode == 'all':\n        cursor.execute(\"\"\"\n            UPDATE conversation_history \n            SET deleted = 1\n            WHERE user_id = ?\n        \"\"\", (user_id,))\n    else:\n        cursor.execute(\"\"\"\n            UPDATE conversation_history \n            SET deleted = 1\n            WHERE user_id = ? AND mode = ?\n        \"\"\", (user_id, mode))\n    \n    conn.commit()\n    conn.close()",
    "start_line": 360,
    "end_line": 379,
    "description": "The `mark_conversations_as_deleted` function soft-deletes conversation history entries associated with a specific `user_id` in a database. It takes the `user_id` (integer) and an optional `mode` (string, defaults to 'chat') as input.  Based on the `mode` (either 'chat', 'rag', or 'all'), the function updates the `deleted` flag to 1 in the `conversation_history` table.  If mode is 'all', all conversations for the user are marked as deleted. Otherwise, only conversations matching the user ID and mode are targeted. The function uses a database connection and executes an SQL `UPDATE` statement."
  },
  {
    "type": "function",
    "name": "get_all_users",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def get_all_users() -> List[Dict[str, Any]]:\n    \"\"\"Get all users for admin panel.\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    cursor.execute(\"\"\"\n        SELECT id, username, requests_left, last_reset_date, is_admin, created_at \n        FROM users \n        ORDER BY created_at DESC\n    \"\"\")\n    \n    rows = cursor.fetchall()\n    conn.close()\n    \n    return [dict(row) for row in rows]",
    "start_line": 382,
    "end_line": 396,
    "description": "The `get_all_users` function retrieves user data from a database for administrative purposes.  It connects to the database, executes an SQL `SELECT` query to fetch user details (ID, username, requests left, last reset date, admin status, and creation date), orders them by creation date in descending order, and fetches all results. The function then converts each row into a dictionary and returns a list of these dictionaries, representing the user data.  No parameters are needed.  The return value is a list of user dictionaries."
  },
  {
    "type": "function",
    "name": "get_user_conversations",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def get_user_conversations(user_id: int, include_deleted: bool = False) -> List[Dict[str, Any]]:\n    \"\"\"Get all conversations for a specific user (admin function).\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    if include_deleted:\n        cursor.execute(\"\"\"\n            SELECT * FROM conversation_history \n            WHERE user_id = ? \n            ORDER BY timestamp DESC\n        \"\"\", (user_id,))\n    else:\n        cursor.execute(\"\"\"\n            SELECT * FROM conversation_history \n            WHERE user_id = ? AND deleted = 0\n            ORDER BY timestamp DESC\n        \"\"\", (user_id,))\n    \n    rows = cursor.fetchall()\n    conn.close()\n    \n    return [dict(row) for row in rows]",
    "start_line": 399,
    "end_line": 420,
    "description": "The `get_user_conversations` function retrieves a user's conversation history from a database. It takes the `user_id` (integer) as input, and optionally `include_deleted` (boolean, defaults to False) to specify whether to include deleted conversations. The function queries the `conversation_history` table using a SQL `SELECT` statement, applying a filter on the `user_id` and potentially the `deleted` flag. It returns a list of dictionaries, where each dictionary represents a conversation, ordered by timestamp in descending order. The function uses a database connection and cursor to fetch and format the data."
  },
  {
    "type": "function",
    "name": "get_db_stats",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def get_db_stats() -> Dict[str, Any]:\n    \"\"\"Get database statistics for admin panel.\"\"\"\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    \n    stats = {}\n    \n    # Total users\n    cursor.execute(\"SELECT COUNT(*) as count FROM users\")\n    stats['total_users'] = cursor.fetchone()['count']\n    \n    # Total conversations\n    cursor.execute(\"SELECT COUNT(*) as count FROM conversation_history WHERE deleted = 0\")\n    stats['total_conversations'] = cursor.fetchone()['count']\n    \n    # Deleted conversations\n    cursor.execute(\"SELECT COUNT(*) as count FROM conversation_history WHERE deleted = 1\")\n    stats['deleted_conversations'] = cursor.fetchone()['count']\n    \n    # Conversations by mode\n    cursor.execute(\"\"\"\n        SELECT mode, COUNT(*) as count \n        FROM conversation_history \n        WHERE deleted = 0\n        GROUP BY mode\n    \"\"\")\n    stats['conversations_by_mode'] = {row['mode']: row['count'] for row in cursor.fetchall()}\n    \n    conn.close()\n    return stats",
    "start_line": 423,
    "end_line": 452,
    "description": "The `get_db_stats` function retrieves various statistics from a database, primarily for an administrative panel. It connects to the database, executes SQL queries to count users, total conversations (including deleted ones), and conversations grouped by mode. The function returns a dictionary containing these statistics, including counts for `total_users`, `total_conversations`, `deleted_conversations`, and a dictionary `conversations_by_mode` mapping conversation modes to their respective counts. The function uses standard SQL `COUNT` and `GROUP BY` operations."
  },
  {
    "type": "function",
    "name": "image_to_base64",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def image_to_base64(image_path: str) -> str:\n    \"\"\"Convert an image file to base64 string.\"\"\"\n    with open(image_path, 'rb') as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')",
    "start_line": 544,
    "end_line": 547,
    "description": "The `image_to_base64` function converts an image file into its base64 string representation.  It takes the image file path (`image_path`, a string) as input. Inside the function, the image is read in binary mode. The function then uses `base64.b64encode` to encode the image's binary content, and  decodes the resulting bytes into a UTF-8 string before returning it. The function simplifies image handling by providing a string representation suitable for embedding in other formats or transmitting over networks."
  },
  {
    "type": "function",
    "name": "image_file_to_base64",
    "filepath": "app/database.py",
    "filename": "database.py",
    "content": "def image_file_to_base64(uploaded_file) -> str:\n    \"\"\"Convert an uploaded Streamlit file to base64 string.\"\"\"\n    return base64.b64encode(uploaded_file.getvalue()).decode('utf-8')",
    "start_line": 550,
    "end_line": 552,
    "description": "The `image_file_to_base64` function converts an image file uploaded via Streamlit into its Base64 encoded string representation. It takes a single parameter, `uploaded_file`, which is the Streamlit file object. The function utilizes `uploaded_file.getvalue()` to access the file's content as bytes. This content is then encoded into Base64 using `base64.b64encode()`. Finally, the resulting bytes are decoded into a UTF-8 string, which is then returned. This string can be readily used for display or transfer purposes."
  },
  {
    "type": "function",
    "name": "log_event",
    "filepath": "app/streamlit_app.py",
    "filename": "streamlit_app.py",
    "content": "def log_event(event: str, level: str = \"info\", **details) -> None:\n    \"\"\"Log structured events for server-side debugging.\"\"\"\n    user = st.session_state.get('username', 'anonymous')\n    payload = {\"event\": event, \"user\": user}\n    payload.update({k: v for k, v in details.items() if v is not None})\n    message = \" | \".join(f\"{key}={value}\" for key, value in payload.items())\n    log_method = getattr(logger, level.lower(), logger.info)\n    log_method(message)",
    "start_line": 41,
    "end_line": 48,
    "description": "The `log_event` function facilitates structured logging for server-side debugging. It logs events with associated details.  It accepts an `event` string and optional `level` (default: \"info\") specifying the log severity.  It retrieves the current user from `st.session_state` and constructs a payload, including event details passed as keyword arguments.  It then formats the payload into a message string and uses a logging method (e.g., `logger.info`, `logger.debug`) chosen based on the specified log level, logging the formatted message. The return value is `None`."
  },
  {
    "type": "function",
    "name": "get_readme_content",
    "filepath": "app/streamlit_app.py",
    "filename": "streamlit_app.py",
    "content": "def get_readme_content() -> str:\n    readme_path = Path(__file__).parent.parent / \"README.md\"\n    try:\n        return readme_path.read_text(encoding=\"utf-8\")\n    except FileNotFoundError:\n        return \"README.md not found.\"",
    "start_line": 52,
    "end_line": 57,
    "description": "The `get_readme_content` function retrieves the contents of the project's README file. It dynamically determines the README file's path relative to the current script.  It attempts to read the file, handling potential `FileNotFoundError` exceptions. If the README exists, its content (a string) is returned, encoded in UTF-8.  If the README is not found, the function returns the string \"README.md not found.\""
  },
  {
    "type": "function",
    "name": "get_movies_dataset",
    "filepath": "app/streamlit_app.py",
    "filename": "streamlit_app.py",
    "content": "def get_movies_dataset() -> list[dict]:\n    return load_movies()",
    "start_line": 61,
    "end_line": 62,
    "description": "The `get_movies_dataset` function provides access to a list of movie data. It utilizes an internal function, `load_movies()`, to retrieve this data.  The function takes no parameters and directly returns the output of `load_movies()`. The return value is a list of dictionaries, where each dictionary likely represents a single movie and holds its associated information (e.g., title, year, genre).  The exact content of the movie data depends on the implementation of the `load_movies()` function."
  },
  {
    "type": "function",
    "name": "render_alt_page",
    "filepath": "app/streamlit_app.py",
    "filename": "streamlit_app.py",
    "content": "def render_alt_page(title: str, content_renderer, total_content_sections: int = 1):\n    st.title(title)\n    col = st.columns([1])[0]\n    if col.button(\"\u2b05 Back to main app\", type=\"primary\"):\n        st.session_state.show_readme_panel = False\n        st.session_state.show_dataset_panel = False\n        st.rerun()\n    st.divider()\n    content_renderer()\n    st.divider()\n    col_bottom = st.columns([1])[0]\n    if col_bottom.button(\"\u2b05 Back\", key=f\"back_bottom_{title}\", type=\"primary\"):\n        st.session_state.show_readme_panel = False\n        st.session_state.show_dataset_panel = False\n        st.rerun()\n    st.stop()",
    "start_line": 64,
    "end_line": 79,
    "description": "The `render_alt_page` function renders an alternative page within a Streamlit application. It displays a title and includes \"Back\" buttons that navigate the user back to the main application.  It takes the page `title` (string) as input and a `content_renderer` function, responsible for rendering the specific page content. The function uses `st.session_state` to control visibility and `st.rerun` to refresh the app. It also uses `st.stop` to halt execution after the content is displayed."
  },
  {
    "type": "function",
    "name": "get_ollama_models",
    "filepath": "app/streamlit_app.py",
    "filename": "streamlit_app.py",
    "content": "def get_ollama_models():\n    try:\n        response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n        if response.status_code == 200:\n            models = response.json().get(\"models\", [])\n            return [model[\"name\"] for model in models]\n    except:\n        pass\n    return []",
    "start_line": 103,
    "end_line": 111,
    "description": "The `get_ollama_models` function retrieves a list of available language models from a local Ollama server. It sends a GET request to the `/api/tags` endpoint on `localhost:11434` with a 2-second timeout.  Upon a successful request (status code 200), the function parses the JSON response, extracts model names, and returns them as a list of strings. If the request fails or an error occurs, the function returns an empty list, providing basic error handling."
  },
  {
    "type": "function",
    "name": "load_ollama_models",
    "filepath": "app/streamlit_app.py",
    "filename": "streamlit_app.py",
    "content": "def load_ollama_models():\n    if st.session_state.model_type == \"local\":\n        models = get_ollama_models()\n        st.session_state.ollama_models = models\n        if models and not st.session_state.selected_ollama_model:\n            st.session_state.selected_ollama_model = models[0] if models else None",
    "start_line": 113,
    "end_line": 118,
    "description": "The `load_ollama_models` function loads and manages local Ollama models within a Streamlit application. It checks if the user's model type is \"local\". If so, it retrieves available models using `get_ollama_models()`. The retrieved models are stored in `st.session_state.ollama_models`.  Finally, it automatically selects the first model as the default if a model exists and no model is currently selected, setting `st.session_state.selected_ollama_model`. The function uses Streamlit's session state to persist model selections."
  },
  {
    "type": "function",
    "name": "check_and_consume_rate_limit",
    "filepath": "app/streamlit_app.py",
    "filename": "streamlit_app.py",
    "content": "def check_and_consume_rate_limit() -> Tuple[bool, str]:\n    \"\"\"\n    Check and consume rate limit for current user.\n    Returns (allowed, message)\n    \"\"\"\n    user_id = get_current_user_id()\n    if not user_id:\n        return False, \"User not logged in\"\n    \n    # No rate limit for custom Gemini API or local models\n    if st.session_state.model_type in [\"custom_gemini\", \"local\"]:\n        return True, \"\"\n    \n    is_system_api = st.session_state.model_type == \"API\"\n    allowed, requests_left = check_rate_limit(user_id, is_system_api)\n    \n    if not allowed:\n        return False, f\"Daily limit reached. You have used all 50 requests for today. Please try again tomorrow or use local API (Ollama) or your own Gemini API key.\"\n    \n    # Consume the rate limit\n    if is_system_api:\n        success = consume_rate_limit(user_id, is_system_api)\n        if not success:\n            return False, \"Failed to consume rate limit. Please try again.\"\n    \n    return True, \"\"",
    "start_line": 120,
    "end_line": 145,
    "description": "The `check_and_consume_rate_limit` function manages user request limits. It verifies if a user is logged in, and if the model type is not \"custom_gemini\" or \"local\", it checks the rate limit based on the user ID and model type (API or not). The function returns a tuple: `(allowed, message)`. If the user is allowed, the function consumes a request from the daily quota (50 requests for the system API) and returns `(True, \"\")`. Otherwise, it provides an informative message indicating the limit has been reached, the models that don't have rate limit or if the request could not be processed, the function returns `(False, message)`."
  },
  {
    "type": "function",
    "name": "save_chat_history",
    "filepath": "app/streamlit_app.py",
    "filename": "streamlit_app.py",
    "content": "def save_chat_history(query_type: str, query_text: str = None, query_image_base64: str = None, \n                      response_text: str = None, response_results: str = None):\n    \"\"\"Save chat history for current user.\"\"\"\n    user_id = get_current_user_id()\n    if not user_id:\n        return\n    \n    model_type = st.session_state.model_type\n    if model_type == \"local\" and st.session_state.selected_ollama_model:\n        model_type = f\"local:{st.session_state.selected_ollama_model}\"\n    elif model_type == \"custom_gemini\":\n        model_type = \"gemini:custom\"\n    \n    add_chat_history(\n        user_id=user_id,\n        query_type=query_type,\n        query_text=query_text,\n        query_image_base64=query_image_base64,\n        response_text=response_text,\n        response_results=response_results,\n        model_type=model_type\n    )",
    "start_line": 147,
    "end_line": 168,
    "description": "The `save_chat_history` function saves a user's chat interaction to a database.  It retrieves the current user's ID and model type from the session state.  Key parameters include the `query_type`, `query_text`, `query_image_base64`, `response_text`, and `response_results`, representing the interaction's components. The function dynamically determines the `model_type`, handling \"local\" and \"custom_gemini\" models.  It then calls the `add_chat_history` function to persist the data, if a user ID exists."
  },
  {
    "type": "function",
    "name": "check_rate_limit_for_api_call",
    "filepath": "app/streamlit_app.py",
    "filename": "streamlit_app.py",
    "content": "def check_rate_limit_for_api_call() -> bool:\n    \"\"\"Check rate limit before making a SYSTEM API call. Returns True if allowed.\"\"\"\n    user_id = get_current_user_id()\n    if not user_id:\n        return False\n    \n    # No rate limit for custom Gemini API or local models\n    if st.session_state.model_type in [\"custom_gemini\", \"local\"]:\n        return True\n\n    is_system_api = st.session_state.model_type == \"API\"\n    if not is_system_api:\n        return True  # No limit for local API\n    \n    allowed, _ = check_rate_limit(user_id, is_system_api)\n    if allowed:\n        consume_rate_limit(user_id, is_system_api)\n    return allowed",
    "start_line": 170,
    "end_line": 187,
    "description": "The `check_rate_limit_for_api_call` function determines if an API call is permitted based on rate limiting. It retrieves the current user's ID. It bypasses rate limiting for custom Gemini and local models. For system API calls, it checks if the user is allowed using `check_rate_limit`. If allowed, `consume_rate_limit` is called to decrement the user's rate limit. The function returns `True` if the API call is permitted, and `False` otherwise."
  },
  {
    "type": "function",
    "name": "_render_readme",
    "filepath": "app/streamlit_app.py",
    "filename": "streamlit_app.py",
    "content": "    def _render_readme():\n        st.markdown(get_readme_content())",
    "start_line": 340,
    "end_line": 341,
    "description": "The `_render_readme` function displays the content of a README file within a Streamlit application. It retrieves the README content using the `get_readme_content()` function (implementation not shown) and then utilizes `st.markdown()` to render the content as Markdown.  There are no parameters passed to this function, and it doesn't return any explicit values. Its primary purpose is to dynamically display a README file within the user interface, providing documentation or introductory information to the user."
  },
  {
    "type": "function",
    "name": "_render_dataset",
    "filepath": "app/streamlit_app.py",
    "filename": "streamlit_app.py",
    "content": "    def _render_dataset():\n        st.caption(\"Full contents of data/movies.json\")\n        st.json({\"movies\": get_movies_dataset()})",
    "start_line": 345,
    "end_line": 347,
    "description": "The `_render_dataset` function displays the entire contents of a movie dataset within a Streamlit application. It retrieves the movie data using the `get_movies_dataset()` function. It then presents this data formatted as a JSON object, using `st.json` for clear visualization. A caption \"Full contents of data/movies.json\" is added to provide context for the displayed dataset. This function effectively provides a way to inspect the raw movie data used by the application."
  },
  {
    "type": "function",
    "name": "get_gemini_client",
    "filepath": "app/model_handler.py",
    "filename": "model_handler.py",
    "content": "def get_gemini_client(api_key: str = None):\n    if not api_key:\n        api_key = os.getenv(\"gemini_api_key\") or os.getenv(\"GEMINI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"Gemini API key not found in environment variables or provided as argument\")\n    client = genai.Client(api_key=api_key)\n    return client",
    "start_line": 13,
    "end_line": 19,
    "description": "The `get_gemini_client` function initializes and returns a Gemini API client. It first attempts to retrieve the API key, prioritizing the provided `api_key` argument. If no argument is provided, it searches for the key in environment variables (\"gemini_api_key\" or \"GEMINI_API_KEY\"). If the API key is not found, a `ValueError` is raised.  The function then instantiates a `genai.Client` object using the retrieved API key and returns this client object, ready for interaction with the Gemini API."
  },
  {
    "type": "function",
    "name": "generate_with_gemini",
    "filepath": "app/model_handler.py",
    "filename": "model_handler.py",
    "content": "def generate_with_gemini(prompt: str, api_key: str = None) -> str:\n    try:\n        client = get_gemini_client(api_key)\n        model = \"gemini-2.0-flash\"\n        response = client.models.generate_content(model=model, contents=prompt)\n        return response.text or \"\"\n    except Exception as e:\n        error_msg = str(e)\n        if \"400\" in error_msg or \"API key\" in error_msg or \"403\" in error_msg:\n            raise InvalidAPIKeyError(f\"Invalid Gemini API Key or permissions error: {error_msg}\")\n        raise e",
    "start_line": 21,
    "end_line": 31,
    "description": "The `generate_with_gemini` function uses the Google Gemini API to generate text. It takes a `prompt` (string) as input and optionally an `api_key`.  Internally, it utilizes the \"gemini-2.0-flash\" model to process the prompt.  The function returns the generated text as a string. Error handling is included to catch potential issues during the API call, particularly invalid API keys or permission problems, raising an `InvalidAPIKeyError` in such cases."
  },
  {
    "type": "function",
    "name": "generate_with_ollama",
    "filepath": "app/model_handler.py",
    "filename": "model_handler.py",
    "content": "def generate_with_ollama(prompt: str, model_name: str) -> str:\n    url = \"http://localhost:11434/api/generate\"\n    payload = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"stream\": False\n    }\n    response = requests.post(url, json=payload, timeout=60)\n    if response.status_code != 200:\n        raise Exception(f\"Ollama API error: {response.status_code} - {response.text}\")\n    result = response.json()\n    return result.get(\"response\", \"\")",
    "start_line": 33,
    "end_line": 44,
    "description": "The `generate_with_ollama` function interacts with a local Ollama server to generate text based on a given prompt. It takes a `prompt` (the input text) and `model_name` (the Ollama model to use) as input. It sends a POST request to the Ollama API, providing the prompt and model selection. The function then parses the JSON response and returns the generated text, accessed from the \"response\" key, or an empty string if there's no response. Error handling includes raising an exception for non-200 HTTP status codes, indicating API failure."
  },
  {
    "type": "function",
    "name": "generate_response",
    "filepath": "app/model_handler.py",
    "filename": "model_handler.py",
    "content": "def generate_response(query: str, results: list[dict], model_type: str = \"API\", ollama_model: str = None, api_key: str = None) -> str:\n    prompt = f\"\"\"Answer the question or provide information based on the provided documents. This should be tailored to Hoopla users. Hoopla is a movie streaming service. Respond without any bolding, italics, or other markdown. Just the text in points if neccessary.\n\n    Query: {query}\n\n    Documents:\n    {results}\n\n    Provide a comprehensive answer that addresses the query:\"\"\"\n    \n    if model_type == \"local\" and ollama_model:\n        return generate_with_ollama(prompt, ollama_model)\n    else:\n        return generate_with_gemini(prompt, api_key)",
    "start_line": 46,
    "end_line": 59,
    "description": "The `generate_response` function crafts answers to user queries about the Hoopla movie streaming service by leveraging either a local LLM (via Ollama) or a Gemini API. It constructs a prompt incorporating the user's `query` and relevant document `results`. Depending on the `model_type` and availability of `ollama_model` and `api_key`, it uses either `generate_with_ollama` or `generate_with_gemini` to generate the response, returning a plain text answer tailored for Hoopla users."
  },
  {
    "type": "function",
    "name": "generate_multidoc_summary",
    "filepath": "app/model_handler.py",
    "filename": "model_handler.py",
    "content": "def generate_multidoc_summary(query: str, results: list[dict], model_type: str = \"API\", ollama_model: str = None, api_key: str = None) -> str:\n    prompt = f\"\"\"\n    Provide information useful to this query by synthesizing information from multiple search results in detail.\n    The goal is to provide comprehensive information so that users know what their options are.\n    Your response should be information-dense and concise, with several key pieces of information about the genre, plot, etc. of each movie.\n    This should be tailored to Hoopla users. Hoopla is a movie streaming service.\n    Respond without any bolding, italics, or other markdown. Just the text in points if neccessary.\n    Query: {query}\n    Search Results:\n    {results}\n    Provide a comprehensive 3-4 sentence answer that combines information from multiple sources:\n    \"\"\"\n    \n    if model_type == \"local\" and ollama_model:\n        return generate_with_ollama(prompt, ollama_model)\n    else:\n        return generate_with_gemini(prompt, api_key)",
    "start_line": 61,
    "end_line": 77,
    "description": "The `generate_multidoc_summary` function synthesizes information from multiple search results to answer a user query about movies, tailored for Hoopla users. It constructs a prompt instructing a language model to create a comprehensive, concise summary. Key parameters include the `query` string, a list of `results` (search result dictionaries), the `model_type` (\"API\" or \"local\"), and optional model-specific parameters like `ollama_model` or `api_key`. The function leverages either `generate_with_ollama` (for local models) or `generate_with_gemini` (for API access) to generate the summary, returning the final text as a string."
  },
  {
    "type": "function",
    "name": "generate_citations",
    "filepath": "app/model_handler.py",
    "filename": "model_handler.py",
    "content": "def generate_citations(query: str, results: list[dict], model_type: str = \"API\", ollama_model: str = None, api_key: str = None) -> str:\n    prompt = f\"\"\"Answer the question or provide information based on the provided documents.\n\n    This should be tailored to Hoopla users. Hoopla is a movie streaming service.\n\n    If not enough information is available to give a good answer, say so but give as good of an answer as you can while citing the  sources you have.\n\n    Query: {query}\n\n    Documents:\n    {results}\n\n    Instructions:\n    - Provide a comprehensive answer that addresses the query\n    - Cite sources using [1], [2], etc. format when referencing information. The number inside the braces should be the id field for that result.\n    - If sources disagree, mention the different viewpoints\n    - If the answer isn't in the documents, say \"I don't have enough information\"\n    - Be direct and informative\n    - Respond without any bolding, italics, or other markdown. Just the text in points if neccessary.\n\n\n    Answer:\"\"\"\n    \n    if model_type == \"local\" and ollama_model:\n        return generate_with_ollama(prompt, ollama_model)\n    else:\n        return generate_with_gemini(prompt, api_key)",
    "start_line": 79,
    "end_line": 105,
    "description": "The `generate_citations` function crafts an answer to a user's query about Hoopla streaming based on provided search results. It constructs a prompt incorporating the query and result documents, formatted for an LLM to generate a comprehensive, cited response using the format [1], [2], etc., referencing document IDs.  It leverages either a local Ollama model (specified by `ollama_model`) or Google Gemini via an API key, determined by the `model_type` parameter.  The function returns the generated text answer with citations."
  },
  {
    "type": "function",
    "name": "generate_answer",
    "filepath": "app/model_handler.py",
    "filename": "model_handler.py",
    "content": "def generate_answer(query: str, results: list[dict], model_type: str = \"API\", ollama_model: str = None, api_key: str = None) -> str:\n    prompt = f\"\"\"Answer the following question based on the provided documents. Answer in a SFW manner only.\n\n    Question: {query}\n\n    Documents:\n    {results}\n\n    General instructions:\n    - Answer directly and concisely\n    - Use only information from the documents\n    - If the answer isn't in the documents, say \"I don't have enough information\"\n    - Cite sources when possible\n\n    Guidance on types of questions:\n    - Factual questions: Provide a direct answer\n    - Analytical questions: Compare and contrast information from the documents\n    - Opinion-based questions: Acknowledge subjectivity and provide a balanced view\n\n    Answer:\"\"\"\n    \n    if model_type == \"local\" and ollama_model:\n        return generate_with_ollama(prompt, ollama_model)\n    else:\n        return generate_with_gemini(prompt, api_key)",
    "start_line": 107,
    "end_line": 131,
    "description": "The `generate_answer` function crafts responses to user queries using provided documents. It constructs a prompt incorporating the query and document results. Based on the `model_type` parameter, it then generates an answer. If `model_type` is \"local\" and an `ollama_model` is specified, it utilizes `generate_with_ollama`. Otherwise, it uses `generate_with_gemini` and requires an `api_key`. The returned answer aims to be concise, fact-based, and sourced from the provided documents."
  },
  {
    "type": "function",
    "name": "main",
    "filepath": "cli/hybrid_search_cli.py",
    "filename": "hybrid_search_cli.py",
    "content": "def main() -> None:\n    parser = argparse.ArgumentParser(description=\"Hybrid Search CLI\")\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    normalize_parser = subparsers.add_parser(\"normalize\", help=\"Normalize the scores\")\n    normalize_parser.add_argument(\"values\", metavar=\"N\", type=float, nargs=\"+\",help=\"Values to normalize\")\n \n    weighted_search_parser = subparsers.add_parser(\"weighted-search\", help=\"Normalize the scores\")\n    weighted_search_parser.add_argument(\"query\", type=str, help=\"Query\")\n    weighted_search_parser.add_argument(\"--alpha\", type=float, default =DEFAULT_ALPHA_VALUE,help=\"Alpha Value\")\n    weighted_search_parser.add_argument(\"--limit\", type=int, default=DEFAULT_SEARCH_LIMIT,help=\"Search Limit\")\n\n    rrf_parser = subparsers.add_parser(\"rrf-search\", help=\"Use RRF to Search\")\n    rrf_parser.add_argument(\"query\", type=str, help=\"Query\")\n    rrf_parser.add_argument(\"--k\", type=float, default =DEFAULT_K_VALUE,help=\"K Value\")\n    rrf_parser.add_argument(\"--limit\", type=int, default=DEFAULT_SEARCH_LIMIT,help=\"Search Limit\")\n    rrf_parser.add_argument(\"--enhance\",type=str,choices=[\"spell\",\"rewrite\",\"expand\"],help=\"Query enhancement method\")\n    rrf_parser.add_argument(\"--rerank-method\",type=str,choices=[\"individual\",\"batch\",\"cross_encoder\"],help=\"Re-ranking method\")\n    rrf_parser.add_argument(\"--evaluate\", action=\"store_true\", help=\"Evaluation Flag\")\n\n    args = parser.parse_args()\n\n    match args.command:\n        case \"normalize\":\n            results = normalize_command(args.values)\n            for score in results :\n                print(f\"* {score:.4f}\")\n        case \"weighted-search\":\n            weighted_search_command(args.query, args.alpha, args.limit)\n        case \"rrf-search\" :\n            rrf_search_command(args.query, args.k, args.limit, args.enhance, args.rerank_method, args.evaluate)\n        case _:\n            parser.print_help()\n\n    end_time = time.perf_counter()\n    \n    elapsed_time = end_time - start_time\n    print(f\"Code execution time: {elapsed_time:.4f} seconds\")",
    "start_line": 14,
    "end_line": 51,
    "description": "The `main` function is the entry point for a command-line interface (CLI) for hybrid search operations. It uses `argparse` to define three subcommands: `normalize`, `weighted-search`, and `rrf-search`. Based on the chosen subcommand, it parses the provided arguments, then calls the corresponding function (e.g., `normalize_command`, `weighted_search_command`, or `rrf_search_command`).  `normalize` takes a list of floating-point values as input. `weighted-search` takes a query, alpha value and limit. `rrf-search` accepts a query, k-value, limit, query enhancement method, reranking method, and evaluation flag. Finally, it prints the results (if any) and the execution time."
  },
  {
    "type": "function",
    "name": "main",
    "filepath": "cli/multimodal_search_cli.py",
    "filename": "multimodal_search_cli.py",
    "content": "def main():\n    parser = argparse.ArgumentParser(description=\"Multi Modal Search CLI\")\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    verify_parser = subparsers.add_parser(\"verify_image_embedding\", help=\"Verify Image Embedding\")\n    verify_parser.add_argument(\"image\", type=str, help=\"Relative path of the image\")\n    \n    image_search_parser = subparsers.add_parser(\"image_search\", help=\"Search Using Image\")\n    image_search_parser.add_argument(\"image\", type=str, help=\"Relative path of the image\")\n\n    args = parser.parse_args()\n\n    match args.command:\n        case \"verify_image_embedding\":\n            verify_image_embedding(args.image)\n        case \"image_search\":\n            image_search_command(args.image)\n        case _:\n            parser.print_help()",
    "start_line": 6,
    "end_line": 24,
    "description": "The `main` function serves as the command-line interface (CLI) entry point for a multi-modal search application. It utilizes `argparse` to define and parse command-line arguments.  It sets up subcommands: `verify_image_embedding` (takes an image path) and `image_search` (also takes an image path).  Based on the chosen subcommand, it calls the corresponding function (`verify_image_embedding` or `image_search_command`) with the provided image path argument.  If no valid command is provided, it prints the help message."
  },
  {
    "type": "function",
    "name": "main",
    "filepath": "cli/keyword_search_cli.py",
    "filename": "keyword_search_cli.py",
    "content": "def main() -> None:\n    parser = argparse.ArgumentParser(description=\"Keyword Search CLI\")\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    subparsers.add_parser(\"build\", help=\"Build the inverted index\")\n\n    tf_parser = subparsers.add_parser(\"tf\", help=\"Get the term frequencies\")\n    tf_parser.add_argument(\"doc_id\", type=int, help=\"Document Id\")\n    tf_parser.add_argument(\"term\", type=str, help=\"Term\")\n\n    idf_parser = subparsers.add_parser(\"idf\", help=\"Get the idf\")\n    idf_parser.add_argument(\"term\", type=str, help=\"Term\")\n    \n    tfidf_parser = subparsers.add_parser(\"tfidf\", help=\"Get the idf\")\n    tfidf_parser.add_argument(\"doc_id\", type=int, help=\"Document Id\")\n    tfidf_parser.add_argument(\"term\", type=str, help=\"Term\")\n\n    bm25_idf_parser = subparsers.add_parser('bm25idf', help=\"Get BM25 IDF score for a given term\")\n    bm25_idf_parser.add_argument(\"term\", type=str, help=\"Term to get BM25 IDF score for\")\n\n    bm25_tf_parser = subparsers.add_parser(\"bm25tf\", help=\"Get BM25 TF score for a given document ID and term\")\n    bm25_tf_parser.add_argument(\"doc_id\", type=int, help=\"Document ID\")\n    bm25_tf_parser.add_argument(\"term\", type=str, help=\"Term to get BM25 TF score for\")\n    bm25_tf_parser.add_argument(\"k1\", type=float, nargs='?', default= BM25_K1, help=\"Tunable BM25 K1 parameter\")\n    bm25_tf_parser.add_argument(\"b\", type=float, nargs='?', default=BM25_B, help=\"Tunable BM25 b parameter\")\n\n    bm25search_parser = subparsers.add_parser(\"bm25search\", help=\"Search movies using full BM25 scoring\")\n    bm25search_parser.add_argument(\"query\", type=str, help=\"Search query\")\n    bm25search_parser.add_argument(\"--limit\", type=int, default= DEFAULT_SEARCH_LIMIT, help=\"Limit\")\n\n\n    search_parser = subparsers.add_parser(\"search\", help=\"Search movies\")\n    search_parser.add_argument(\"query\", type=str, help=\"Search query\")\n    search_parser.add_argument(\"--limit\", type=int, default= DEFAULT_SEARCH_LIMIT, help=\"Limit\")\n\n    args = parser.parse_args()\n\n    match args.command:\n        case \"build\":\n            print(\"Building inverted index...\")\n            build_command()\n            print(\"Inverted index built successfully.\")\n        case \"tf\":\n            tf = tf_command(args.doc_id, args.term)\n            print(f\"Term frequency of '{args.term}' in document '{args.doc_id}': {tf}\")\n        case \"idf\":\n            idf = idf_command(args.term)\n            print(f\"Inverse document frequency of '{args.term}': {idf:.2f}\")\n        case \"tfidf\":\n            tf_idf = tf_command(args.doc_id, args.term)*idf_command(args.term)\n            print(f\"TF-IDF score of '{args.term}' in document '{args.doc_id}': {tf_idf:.2f}\")\n        case \"bm25idf\":\n            bm25idf = bm25_idf_command(args.term)\n            print(f\"BM25 IDF score of '{args.term}': {bm25idf:.2f}\")\n        case \"bm25tf\":\n            bm25tf = bm25_tf_command(args.doc_id, args.term)\n            print(f\"BM25 TF score of '{args.term}' in document '{args.doc_id}': {bm25tf:.2f}\")\n        case \"bm25search\":\n            print(f\"Getting top {args.limit} for:\", args.query)\n            results = bm25_search_command(args.query, args.limit)\n            for i, res in enumerate(results, 1):\n                print(f\"{i}. ({res['id']}) {res['title']} - Score: {res['score']:.2f}\")\n        case \"search\":\n            print(\"Searching for:\", args.query)\n            results = search_command(args.query, args.limit)\n            for i, res in enumerate(results, 1):\n                print(f\"{i}. ({res['id']}) {res['title']}\")\n        case _:\n            parser.exit(2, parser.format_help())\n    \n    end_time = time.perf_counter()\n\n    elapsed_time = end_time - start_time\n    print(f\"Code execution time: {elapsed_time:.4f} seconds\")",
    "start_line": 13,
    "end_line": 86,
    "description": "The `main` function serves as the command-line interface (CLI) entry point for a keyword search application. It utilizes `argparse` to define various subcommands like \"build\", \"tf\", \"idf\", \"tfidf\", \"bm25idf\", \"bm25tf\", \"bm25search\", and \"search\". Based on the user's chosen command and provided arguments (e.g., document ID, term, query), it calls corresponding functions to perform tasks such as building an inverted index, calculating term frequencies, and executing searches with both standard and BM25 scoring. It prints the results and execution time."
  },
  {
    "type": "function",
    "name": "main",
    "filepath": "cli/semantic_search_cli.py",
    "filename": "semantic_search_cli.py",
    "content": "def main():\n    parser = argparse.ArgumentParser(description=\"Semantic Search CLI\")\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    subparsers.add_parser(\"verify\", help=\"Verify Model\")\n\n    subparsers.add_parser(\"verify_embeddings\", help=\"Verify Embeddings\")\n\n    embed_text_parser = subparsers.add_parser(\"embed_text\", help=\"Embed Text\")\n    embed_text_parser.add_argument(\"query\", type=str, help=\"Search query\")\n    \n    embed_query_parser = subparsers.add_parser(\"embedquery\", help=\"Embed Query\")\n    embed_query_parser.add_argument(\"query\", type=str, help=\"Search query\")\n    \n    search_parser = subparsers.add_parser(\"search\", help=\"Search Command\")\n    search_parser.add_argument(\"query\", type=str, help=\"Search query\")\n    search_parser.add_argument(\"--limit\", type=int, default= DEFAULT_SEARCH_LIMIT, help=\"Limit\")\n\n    chunk_parser = subparsers.add_parser(\"chunk\", help= \"Chunk the query\")\n    chunk_parser.add_argument(\"query\", type=str, help=\"Search query\")\n    chunk_parser.add_argument(\"--chunk-size\", type=int, default= DEFAULT_CHUNK_SIZE, help=\"chunk size\")\n    chunk_parser.add_argument(\"--overlap\", type=int, default= DEFAULT_OVERLAP_SIZE, help=\"overlap size\")\n\n    semantic_chunk_parser = subparsers.add_parser(\"semantic_chunk\", help= \"Chunk the query\")\n    semantic_chunk_parser.add_argument(\"query\", type=str, help=\"Search query\")\n    semantic_chunk_parser.add_argument(\"--max-chunk-size\", type=int, default= DEFAULT_MAX_CHUNK_SIZE, help=\"max chunk size\")\n    semantic_chunk_parser.add_argument(\"--overlap\", type=int, default= DEFAULT_OVERLAP_SIZE, help=\"overlap size\")\n\n    subparsers.add_parser(\"embed_chunks\", help= \"Embed the chunks\")\n\n    semantic_search_parser = subparsers.add_parser(\"search_chunked\", help=\"Search Command\")\n    semantic_search_parser.add_argument(\"query\", type=str, help=\"Search query\")\n    semantic_search_parser.add_argument(\"--limit\", type=int, default= DEFAULT_SEARCH_LIMIT, help=\"Limit\")\n\n\n    args = parser.parse_args()\n\n    match args.command:\n        case \"verify\":\n            verify_model()\n        case \"verify_embeddings\":\n            verify_embeddings()\n        case \"embed_text\":\n            embed_text(args.query)\n        case \"embedquery\":\n            embed_query_text(args.query)\n        case \"search\":\n            search_command(args.query, args.limit)\n        case \"chunk\":\n            results = chunk_command(args.query, args.chunk_size, args.overlap)\n            print(f\"Chunking {len(args.query)} characters\")\n            for i, res in enumerate(results, 1):\n                print(f\"{i}. {res}\")\n        case \"semantic_chunk\":\n            results = semantic_chunk_command(args.query, args.max_chunk_size, args.overlap)\n            print(f\"Semantically chunking {len(args.query)} characters\")\n            for i, res in enumerate(results, 1):\n                print(f\"{i}. {res}\")\n        case \"embed_chunks\":\n            embed_chunks_command()\n        case \"search_chunked\":\n            results = search_chunked_command(args.query, args.limit)\n            for i, res in enumerate(results, 1):\n                print(f\"\\n{i}. {res['title']} (score: {res['score']:.4f})\")\n                print(f\"   {res['description']}...\")\n        case _:\n            parser.print_help()\n    \n    end_time = time.perf_counter()\n    \n    elapsed_time = end_time - start_time\n    print(f\"Code execution time: {elapsed_time:.4f} seconds\")",
    "start_line": 10,
    "end_line": 81,
    "description": "The `main` function serves as the command-line interface (CLI) entry point for a semantic search application. It utilizes `argparse` to define and parse various subcommands, including `verify`, `embed_text`, `search`, and chunking operations.  Based on the parsed `command` argument, the function dispatches control to different handler functions.  Key parameters include `query`, `limit`, `chunk-size`, and `overlap`, passed to relevant subcommand functions.  It measures and prints the total code execution time."
  },
  {
    "type": "function",
    "name": "main",
    "filepath": "cli/augmented_generation_cli.py",
    "filename": "augmented_generation_cli.py",
    "content": "def main():\n    parser = argparse.ArgumentParser(description=\"Retrieval Augmented Generation CLI\")\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    rag_parser = subparsers.add_parser(\"rag\", help=\"Perform RAG (search + generate answer)\")\n    rag_parser.add_argument(\"query\", type=str, help=\"Search query for RAG\")\n    \n    rag_parser = subparsers.add_parser(\"summarize\", help=\"Perform Multi Document Summary\")\n    rag_parser.add_argument(\"query\", type=str, help=\"Search query for Summarize\")\n    \n    rag_parser = subparsers.add_parser(\"citations\", help=\"Provide Citations via LLM\")\n    rag_parser.add_argument(\"query\", type=str, help=\"Search query to provide Citations\")\n    \n    rag_parser = subparsers.add_parser(\"question\", help=\"Answer Questions via LLM\")\n    rag_parser.add_argument(\"query\", type=str, help=\"Search query to provide answer\")\n\n    args = parser.parse_args()\n\n    match args.command:\n        case \"rag\":\n            rag_command(args.query)\n        case \"summarize\":\n            summarize_command(args.query)\n        case \"citations\":\n            citations_command(args.query)\n        case \"question\":\n            question_command(args.query)\n        case _:\n            parser.print_help()",
    "start_line": 6,
    "end_line": 34,
    "description": "The `main` function serves as the command-line interface (CLI) entry point for a Retrieval Augmented Generation (RAG) system. It uses `argparse` to define subcommands: `rag`, `summarize`, `citations`, and `question`.  Each subcommand accepts a `query` argument (string).  Based on the chosen command, it then calls a corresponding function (`rag_command`, `summarize_command`, `citations_command`, or `question_command`) passing the query. If no valid command is provided, it prints the help message."
  },
  {
    "type": "function",
    "name": "main",
    "filepath": "cli/codebase_rag_cli.py",
    "filename": "codebase_rag_cli.py",
    "content": "def main():\n    parser = argparse.ArgumentParser(description=\"Codebase RAG CLI\")\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    # Index command\n    subparsers.add_parser(\"index\", help=\"Build the codebase index\")\n\n    # Search command\n    search_parser = subparsers.add_parser(\"search\", help=\"Search the codebase\")\n    search_parser.add_argument(\"query\", type=str, help=\"Search query\")\n    search_parser.add_argument(\"--limit\", type=int, default=5, help=\"Number of results to return\")\n\n    args = parser.parse_args()\n\n    if args.command == \"index\":\n        build_codebase_index_command()\n    elif args.command == \"search\":\n        search_codebase_command(args.query, args.limit)\n    else:\n        parser.print_help()",
    "start_line": 10,
    "end_line": 29,
    "description": "The `main` function sets up a command-line interface (CLI) for interacting with a codebase using Retrieval-Augmented Generation (RAG). It utilizes the `argparse` module to define two primary commands: \"index\" and \"search\". The \"index\" command builds an index of the codebase, while the \"search\" command takes a `query` (string) as input and an optional `--limit` (integer, defaults to 5) to control the number of search results. Based on the specified command, it calls the appropriate function (`build_codebase_index_command` or `search_codebase_command`). If no valid command is provided, it prints the help message."
  },
  {
    "type": "function",
    "name": "main",
    "filepath": "cli/evaluation_cli.py",
    "filename": "evaluation_cli.py",
    "content": "def main():\n    parser = argparse.ArgumentParser(description=\"Search Evaluation CLI\")\n    parser.add_argument(\n        \"--limit\",\n        type=int,\n        default=DEFAULT_SEARCH_LIMIT,\n        help=\"Number of results to evaluate (k for precision@k, recall@k)\",\n    )\n\n    args = parser.parse_args()\n    limit = args.limit\n\n    documents = load_movies()\n    hybrid_search = HybridSearch(documents)\n\n    testcases = load_testcases()\n    relevant_retrieved = {}\n\n    for testcase in testcases:\n        relevant_retrieved['number'] = 0\n        relevant_retrieved['retrieved'] = []\n        results = hybrid_search.rrf_search(testcase['query'],DEFAULT_K_VALUE,limit)\n        for result in results:\n            relevant_retrieved['retrieved'].append(result['title'])\n            if result['title'] in testcase['relevant_docs']:\n                relevant_retrieved['number'] += 1\n        precision = relevant_retrieved['number'] / limit\n        recall = relevant_retrieved['number'] / len(testcase['relevant_docs'])\n        f1 = 2 * (precision * recall) / (precision + recall)\n\n        print(f\"\\n- Query: {testcase['query']} \\n\\t- Precision@{limit}: {precision:.4f} \\n\\t- Recall@{limit}: {recall:.4f} \\n\\t- F1 Score@{limit}: {f1:.4f} \\n\\t- Retrieved: {relevant_retrieved['retrieved']} \\n\\t- Relevant: {testcase['relevant_docs']}\")",
    "start_line": 10,
    "end_line": 40,
    "description": "The `main` function evaluates the performance of a hybrid search system. It uses `argparse` to accept a result limit. It loads movie data and test cases, then performs hybrid search on each test case query. For each query, it calculates precision, recall, and F1 score based on retrieved documents and known relevant documents, up to the specified limit. Finally, it prints the query, metrics, retrieved document titles, and the relevant documents for each test case."
  },
  {
    "type": "function",
    "name": "main",
    "filepath": "cli/describe_image_cli.py",
    "filename": "describe_image_cli.py",
    "content": "def main():\n\n    parser = argparse.ArgumentParser(description=\"Describe Image CLI\")\n\n    parser.add_argument(\"--image\", type=str, help=\"Path to the image\")\n    parser.add_argument(\"--query\", type=str, help=\"Query\")\n\n    args = parser.parse_args()\n\n    describe_image_command(args.query, args.image)",
    "start_line": 5,
    "end_line": 14,
    "description": "The `main` function serves as the entry point for a command-line interface (CLI) tool designed to describe images. It utilizes the `argparse` module to handle command-line arguments. It defines two arguments: `--image`, which specifies the image file path, and `--query`, which holds the description query.  After parsing the arguments, the function invokes the `describe_image_command` function, passing the query and image path to perform the image description task. This effectively delegates the core image description logic."
  },
  {
    "type": "function",
    "name": "weighted_search_command",
    "filepath": "cli/lib/hybrid_search.py",
    "filename": "hybrid_search.py",
    "content": "def weighted_search_command(query:str, alpha:float = DEFAULT_ALPHA_VALUE, limit: int = DEFAULT_SEARCH_LIMIT):\n    documents = load_movies()\n    hybrid_search = HybridSearch(documents)\n    results = hybrid_search.weighted_search(query, alpha, limit)\n    for i, res in enumerate(results, 1):\n        print(f\"{i}.\\t{res['title']} \\n\\tHybrid Score: {res['hybrid_score']:.3f} \\n\\t{res['description'][:100]}...\\n\")",
    "start_line": 108,
    "end_line": 113,
    "description": "The `weighted_search_command` function searches a movie dataset based on a user-provided `query`. It employs a `HybridSearch` algorithm, likely combining different search methods.  Key parameters are the search `query`, a weighting factor `alpha` (defaults to `DEFAULT_ALPHA_VALUE`), and a result limit `limit` (defaults to `DEFAULT_SEARCH_LIMIT`).  The function retrieves movie documents, performs the weighted search, and prints the top results, displaying their titles, hybrid scores, and truncated descriptions."
  },
  {
    "type": "function",
    "name": "rrf_search_command",
    "filepath": "cli/lib/hybrid_search.py",
    "filename": "hybrid_search.py",
    "content": "def rrf_search_command(query: str, k: int, limit: int, enhance: str, rerank: str, evaluate: bool) :\n    documents = load_movies()\n    hybrid_search = HybridSearch(documents)\n    \n    if enhance:\n        enhanced_query = enhance_query(query,method=enhance)\n        query = enhanced_query\n    \n    if rerank:\n        results = hybrid_search.rrf_search(query, k, limit*5)\n        results = re_rank(query,results,limit,rerank)\n        return\n    else:\n        results = hybrid_search.rrf_search(query, k, limit)\n\n    for i, res in enumerate(results, 1):\n        print(f\"{i}.\\t{res['title']} \\n\\tRRF Score: {res['rrf_score']:.3f} \\n\\tBM25 Rank: {res['bm25_rank']}, Semantic Rank: {res['semantic_rank']}\\n\\t{res['description'][:100]}...\\n\")\n\n    if evaluate:\n        evaluate_results(query, results)",
    "start_line": 115,
    "end_line": 134,
    "description": "The `rrf_search_command` function searches a movie database using a hybrid search approach, leveraging both BM25 and semantic ranking. It takes a `query` string, top-k results (`k`), result limit (`limit`), query enhancement method (`enhance`), and a reranking strategy (`rerank`) as parameters.  Optionally, the query is enhanced and/or results are reranked.  It then displays the search results, including title, RRF score, and ranks, and optionally evaluates the results if `evaluate` is True. The core algorithm is the Robust Rank Fusion (RRF) method for combining ranking scores."
  },
  {
    "type": "function",
    "name": "rrf_score",
    "filepath": "cli/lib/hybrid_search.py",
    "filename": "hybrid_search.py",
    "content": "def rrf_score(rank, k: int = DEFAULT_K_VALUE):\n    return 1/(k + rank)",
    "start_line": 136,
    "end_line": 137,
    "description": "The `rrf_score` function calculates a Reciprocal Rank Fusion (RRF) score for a given rank. It takes the document's `rank` (position in a ranked list) and an optional `k` parameter, defaulting to `DEFAULT_K_VALUE`.  The function implements the RRF formula: 1 / (k + rank). This score emphasizes higher-ranked documents. It returns a floating-point value representing the RRF score, with higher scores indicating higher relevance."
  },
  {
    "type": "function",
    "name": "hybrid_score",
    "filepath": "cli/lib/hybrid_search.py",
    "filename": "hybrid_search.py",
    "content": "def hybrid_score(bm25_score, semantic_score, alpha=DEFAULT_ALPHA_VALUE):\n    return alpha * bm25_score + (1 - alpha) * semantic_score",
    "start_line": 139,
    "end_line": 140,
    "description": "The `hybrid_score` function calculates a combined score from a BM25 relevance score and a semantic similarity score. It blends these scores using a weighted average. The `alpha` parameter, defaulting to `DEFAULT_ALPHA_VALUE`, controls the weighting, determining the influence of each score. A higher `alpha` emphasizes the BM25 score, while a lower `alpha` prioritizes the semantic score. The function returns the final hybrid score, representing a combined measure of relevance and semantic understanding."
  },
  {
    "type": "function",
    "name": "normalize_command",
    "filepath": "cli/lib/hybrid_search.py",
    "filename": "hybrid_search.py",
    "content": "def normalize_command(scores: list[float]) -> list[float]:\n    if len(scores) == 0:\n        return []\n    sorted_scores = sorted(scores)\n    max_score = sorted_scores[-1]\n    min_score = sorted_scores[0]\n    results =[]\n    for score in scores:\n        if min_score == max_score:\n            return [1.0] * len(scores)\n        results.append((score-min_score)/(max_score-min_score))\n    return results",
    "start_line": 142,
    "end_line": 153,
    "description": "The `normalize_command` function normalizes a list of numerical scores. It takes a list of floats, `scores`, as input. It returns a new list of floats, where each score is scaled between 0 and 1.  It first sorts the scores to find the minimum and maximum values.  If all scores are the same, it returns a list of 1.0s. Otherwise, it linearly maps each original score to a normalized value using the formula `(score - min_score) / (max_score - min_score)`."
  },
  {
    "type": "function",
    "name": "__init__",
    "filepath": "cli/lib/hybrid_search.py",
    "filename": "hybrid_search.py",
    "content": "    def __init__(self, documents):\n        self.documents = documents\n        self.doc_map = {doc['id']: doc for doc in documents}\n        self.semantic_search = ChunkedSemanticSearch()\n        self.semantic_search.load_or_create_chunk_embeddings(documents)\n\n        self.idx = InvertedIndex()\n        if not os.path.exists(self.idx.index_path):\n            self.idx.build()\n            self.idx.save()",
    "start_line": 24,
    "end_line": 33,
    "description": "The `__init__` function initializes a document retrieval system. It takes a list of document dictionaries as input. It stores these documents and creates a mapping for quick access by document ID.  It utilizes a `ChunkedSemanticSearch` object to manage document embeddings, loading or creating them.  An `InvertedIndex` is also initialized. If the index doesn't already exist on disk, it's built and saved for efficient keyword-based search.  This prepares the system for semantic and keyword-based document retrieval."
  },
  {
    "type": "function",
    "name": "_bm25_search",
    "filepath": "cli/lib/hybrid_search.py",
    "filename": "hybrid_search.py",
    "content": "    def _bm25_search(self, query, limit):\n        self.idx.load()\n        return self.idx.bm25_search(query, limit)",
    "start_line": 35,
    "end_line": 37,
    "description": "The `_bm25_search` function performs a BM25 search on a pre-indexed corpus. It takes a search `query` (string) and a `limit` (integer) as input, specifying the maximum number of results to return.  It first loads the index data using `self.idx.load()`. Then, it utilizes the BM25 algorithm implemented in `self.idx.bm25_search()` to rank documents based on their relevance to the query, returning a list of ranked document identifiers up to the specified `limit`."
  },
  {
    "type": "function",
    "name": "weighted_search",
    "filepath": "cli/lib/hybrid_search.py",
    "filename": "hybrid_search.py",
    "content": "    def weighted_search(self, query, alpha, limit=DEFAULT_SEARCH_LIMIT):\n        keyword_res = self._bm25_search(query, limit=limit * 500)\n        semantic_res = self.semantic_search.search_chunks(query, limit=limit * 500)\n\n        bm25_scores_dict = {doc['id']: doc['score'] for doc in keyword_res}\n        semantic_scores_dict = {doc['id']: doc['score'] for doc in semantic_res}\n\n        norm_bm25_scores = normalize_command(list(bm25_scores_dict.values()))\n        norm_semantic_scores = normalize_command(list(semantic_scores_dict.values()))\n        \n        norm_bm25_dict = {doc_id: score for doc_id, score in zip(bm25_scores_dict.keys(), norm_bm25_scores)}\n        norm_semantic_dict = {doc_id: score for doc_id, score in zip(semantic_scores_dict.keys(), norm_semantic_scores)}\n\n        all_ids = set(norm_bm25_dict.keys()).union(norm_semantic_dict.keys())\n        hybrid_scores = {}\n        for doc_id in all_ids:\n            bm_score = norm_bm25_dict.get(doc_id, 0.0)\n            sem_score = norm_semantic_dict.get(doc_id, 0.0)\n            hybrid_scores[doc_id] = hybrid_score(bm_score, sem_score, alpha)\n\n        top_doc_ids = heapq.nlargest(limit, hybrid_scores, key=hybrid_scores.get)\n\n        results = []\n        for doc_id in top_doc_ids:\n            document = self.doc_map[doc_id].copy()\n            document['hybrid_score'] = hybrid_scores[doc_id]\n            results.append(document)\n\n        return results",
    "start_line": 39,
    "end_line": 67,
    "description": "The `weighted_search` function performs a hybrid search combining keyword (BM25) and semantic search results.  It accepts a `query`, a weighting factor `alpha` (for blending results), and a `limit` for the final output.  First, it retrieves expanded results from both search types. It normalizes and combines the scores using a weighted sum controlled by `alpha`. The function then returns a list of documents, limited by `limit`, sorted by the calculated hybrid score."
  },
  {
    "type": "function",
    "name": "rrf_search",
    "filepath": "cli/lib/hybrid_search.py",
    "filename": "hybrid_search.py",
    "content": "    def rrf_search(self, query, k: int = DEFAULT_K_VALUE, limit: int = 2 * DEFAULT_SEARCH_LIMIT):\n        \n        logging.info(f\"Original Query: {query}\")\n\n        keyword_res = self._bm25_search(query, limit=limit * 500)\n        semantic_res = self.semantic_search.search_chunks(query, limit=limit * 500)\n\n        rrf_scores = defaultdict(float)\n        \n        doc_ranks_bm25 = {}\n        doc_ranks_semantic = {}\n\n        for i, doc in enumerate(keyword_res, 1):\n            doc_id = doc['id']\n            rank = i\n            rrf_scores[doc_id] += rrf_score(rank, k)\n            doc_ranks_bm25[doc_id] = rank\n\n        for i, doc in enumerate(semantic_res, 1):\n            doc_id = doc['id']\n            rank = i\n            rrf_scores[doc_id] += rrf_score(rank, k)\n            doc_ranks_semantic[doc_id] = rank\n\n        top_doc_ids = heapq.nlargest(limit, rrf_scores, key=rrf_scores.get)\n\n        results = []\n        for doc_id in top_doc_ids:\n            document = self.doc_map[doc_id].copy()\n            document[\"rrf_score\"] = rrf_scores[doc_id]\n            \n            document[\"bm25_rank\"] = doc_ranks_bm25.get(doc_id)\n            document[\"semantic_rank\"] = doc_ranks_semantic.get(doc_id)\n            \n            results.append(document)\n        \n        # logging.info(f\"RRF_SEARCH Results: {results}\")\n        return results",
    "start_line": 69,
    "end_line": 106,
    "description": "The `rrf_search` function performs a search combining keyword (BM25) and semantic search results using Reciprocal Rank Fusion (RRF). It takes a `query` string, an optional `k` value (defaulting to `DEFAULT_K_VALUE` for RRF scoring), and a `limit` (defaulting to `2 * DEFAULT_SEARCH_LIMIT`) to determine the number of results.  It performs BM25 and semantic searches, calculates RRF scores for each document by summing scores from both search methods, and then returns a list of the top `limit` documents, augmented with their RRF score, BM25 rank, and semantic rank."
  },
  {
    "type": "function",
    "name": "generate_response",
    "filepath": "cli/lib/augmented_generation.py",
    "filename": "augmented_generation.py",
    "content": "def generate_response(query: str, results: list[dict]) -> str:\n    \n    prompt = f\"\"\"Answer the question or provide information based on the provided documents. This should be tailored to Hoopla users. Hoopla is a movie streaming service. Respond without any bolding, italics, or other markdown. Just the text in points if neccessary.\n\n    Query: {query}\n\n    Documents:\n    {results}\n\n    Provide a comprehensive answer that addresses the query:\"\"\" \n\n    response = client.models.generate_content(model=model, contents=prompt)\n    return response.text",
    "start_line": 16,
    "end_line": 28,
    "description": "The `generate_response` function crafts a natural language response to a user query, specifically tailored for Hoopla streaming service users.  It takes a user `query` (string) and a list of `results` (dictionaries) as input, which represent relevant documents. It constructs a prompt incorporating the query and documents for a large language model. The function then leverages `client.models.generate_content` using the provided `model` to produce a comprehensive, markdown-free textual answer and returns the resulting string."
  },
  {
    "type": "function",
    "name": "generate_multidoc_summary",
    "filepath": "cli/lib/augmented_generation.py",
    "filename": "augmented_generation.py",
    "content": "def generate_multidoc_summary(query: str, results: list[dict]) -> str:\n    prompt = f\"\"\"\n    Provide information useful to this query by synthesizing information from multiple search results in detail.\n    The goal is to provide comprehensive information so that users know what their options are.\n    Your response should be information-dense and concise, with several key pieces of information about the genre, plot, etc. of each movie.\n    This should be tailored to Hoopla users. Hoopla is a movie streaming service.\n    Respond without any bolding, italics, or other markdown. Just the text in points if neccessary.\n    Query: {query}\n    Search Results:\n    {results}\n    Provide a comprehensive 3-4 sentence answer that combines information from multiple sources:\n    \"\"\"\n\n    response = client.models.generate_content(model=model, contents=prompt)\n    return response.text",
    "start_line": 30,
    "end_line": 44,
    "description": "The `generate_multidoc_summary` function synthesizes information from multiple search results related to a user's query about movies on Hoopla. It constructs a detailed, concise summary for the user. It leverages a prompt to a language model that instructs it to combine information from the provided `results` (a list of dictionaries, presumably search snippets) to answer the `query`. The function returns the generated text response, tailored for Hoopla users, avoiding markdown formatting."
  },
  {
    "type": "function",
    "name": "generate_citations",
    "filepath": "cli/lib/augmented_generation.py",
    "filename": "augmented_generation.py",
    "content": "def generate_citations(query: str, results: list[dict]) -> str:\n    prompt = prompt = f\"\"\"Answer the question or provide information based on the provided documents.\n\n    This should be tailored to Hoopla users. Hoopla is a movie streaming service.\n\n    If not enough information is available to give a good answer, say so but give as good of an answer as you can while citing the  sources you have.\n\n    Query: {query}\n\n    Documents:\n    {results}\n\n    Instructions:\n    - Provide a comprehensive answer that addresses the query\n    - Cite sources using [1], [2], etc. format when referencing information. The number inside the braces should be the id field for that result.\n    - If sources disagree, mention the different viewpoints\n    - If the answer isn't in the documents, say \"I don't have enough information\"\n    - Be direct and informative\n    - Respond without any bolding, italics, or other markdown. Just the text in points if neccessary.\n\n\n    Answer:\"\"\"\n\n    response = client.models.generate_content(model=model, contents=prompt)\n    return response.text",
    "start_line": 46,
    "end_line": 70,
    "description": "The `generate_citations` function crafts an answer to a user's query about movies, tailored for Hoopla users, leveraging provided documents. It constructs a prompt for a large language model (LLM), embedding the query and search results.  Crucially, it instructs the LLM to cite its sources from the results (numbered by ID) within the answer and handle conflicting information appropriately. The function returns the generated text response from the LLM, containing the answer and citations."
  },
  {
    "type": "function",
    "name": "generate_answer",
    "filepath": "cli/lib/augmented_generation.py",
    "filename": "augmented_generation.py",
    "content": "def generate_answer(query: str, results: list[dict]) -> str:\n    prompt = f\"\"\"Answer the following question based on the provided documents.\n\n    Question: {query}\n\n    Documents:\n    {results}\n\n    General instructions:\n    - Answer directly and concisely\n    - Use only information from the documents\n    - If the answer isn't in the documents, say \"I don't have enough information\"\n    - Cite sources when possible\n\n    Guidance on types of questions:\n    - Factual questions: Provide a direct answer\n    - Analytical questions: Compare and contrast information from the documents\n    - Opinion-based questions: Acknowledge subjectivity and provide a balanced view\n\n    Answer:\"\"\"\n\n    response = client.models.generate_content(model=model, contents=prompt)\n    return response.text",
    "start_line": 72,
    "end_line": 94,
    "description": "The `generate_answer` function crafts a natural language response to a given `query` using a set of `results` (documents) as context. It constructs a detailed prompt, incorporating the query and documents, while specifying instructions for the AI model on how to formulate the response. The prompt guides the model to answer concisely, cite sources, and handle various question types, including factual, analytical, and opinion-based queries. The function utilizes a pre-defined `client` and `model` to generate an answer based on the prompt, then returns the generated text."
  },
  {
    "type": "function",
    "name": "get_results",
    "filepath": "cli/lib/augmented_generation.py",
    "filename": "augmented_generation.py",
    "content": "def get_results(query: str) -> list[dict]:\n    documents = load_movies()\n    hybrid_search = HybridSearch(documents)\n\n    results = hybrid_search.rrf_search(query, limit=DEFAULT_SEARCH_LIMIT)\n    return results",
    "start_line": 96,
    "end_line": 101,
    "description": "The `get_results` function searches for movie documents based on a given query string. It utilizes a `HybridSearch` object, initialized with movie documents loaded from `load_movies()`. The core logic employs a Relevance Ranking Fusion (RRF) search algorithm, executed via `hybrid_search.rrf_search()`. This function takes the `query` and a search limit (defaulting to `DEFAULT_SEARCH_LIMIT`) as input.  It returns a list of dictionaries, where each dictionary represents a search result containing information about a matching movie."
  },
  {
    "type": "function",
    "name": "rag_command",
    "filepath": "cli/lib/augmented_generation.py",
    "filename": "augmented_generation.py",
    "content": "def rag_command(query: str) -> None:\n    results = get_results(query)\n    response = generate_response(query, results)\n\n    print(\"Search Results: \")\n    for res in results:\n        print(f\"\\n\\t-{res['title']}\")\n    \n    print(f\"\\n\\nRAG Response:\\n{response}\")",
    "start_line": 103,
    "end_line": 111,
    "description": "The `rag_command` function implements a Retrieval-Augmented Generation (RAG) pipeline. It takes a `query` string as input, retrieves relevant search results using `get_results`, and generates a natural language response incorporating those results via `generate_response`.  The function then prints the search results (title only) followed by the generated RAG response to the console.  It doesn't return a value, operating solely through side effects (printing)."
  },
  {
    "type": "function",
    "name": "summarize_command",
    "filepath": "cli/lib/augmented_generation.py",
    "filename": "augmented_generation.py",
    "content": "def summarize_command(query: str) -> None:\n    results = get_results(query)\n    response = generate_multidoc_summary(query, results)\n\n    print(\"Search Results: \")\n    for res in results:\n        print(f\"\\n\\t-{res['title']}\")\n    \n    print(f\"\\n\\nLLM Summary:\\n{response}\")",
    "start_line": 113,
    "end_line": 121,
    "description": "The `summarize_command` function retrieves and summarizes search results based on a user-provided query. It uses the `get_results` function to fetch search results.  It then employs `generate_multidoc_summary` to generate a comprehensive summary of those results. Finally, it prints the titles of the search results, followed by the LLM-generated summary. The function takes the search `query` (a string) as input and returns nothing (`None`), printing the outputs to the console."
  },
  {
    "type": "function",
    "name": "citations_command",
    "filepath": "cli/lib/augmented_generation.py",
    "filename": "augmented_generation.py",
    "content": "def citations_command(query: str) -> None:\n    results = get_results(query)\n    response = generate_citations(query, results)\n\n    print(\"Search Results: \")\n    for res in results:\n        print(f\"\\n\\t-{res['title']}\")\n    \n    print(f\"\\n\\nLLM Answer:\\n{response}\")",
    "start_line": 123,
    "end_line": 131,
    "description": "The `citations_command` function retrieves and presents search results along with an LLM-generated answer. It takes a `query` string as input. First, it uses `get_results` to fetch search results based on the `query`. Then, it calls `generate_citations` to produce an LLM-based response, incorporating the search results. Finally, it prints the titles of the search results and the LLM's generated response to the console. The function returns nothing (`None`)."
  },
  {
    "type": "function",
    "name": "question_command",
    "filepath": "cli/lib/augmented_generation.py",
    "filename": "augmented_generation.py",
    "content": "def question_command(query: str) -> None:\n    results = get_results(query)\n    response = generate_answer(query, results)\n\n    print(\"Search Results: \")\n    for res in results:\n        print(f\"\\n\\t-{res['title']}\")\n    \n    print(f\"\\n\\nAnswer:\\n{response}\")",
    "start_line": 133,
    "end_line": 141,
    "description": "The `question_command` function processes a user's question (a string). It first retrieves search results using the `get_results` function based on the provided query. Then, it generates an answer using `generate_answer`, incorporating the query and retrieved results. The function prints the search results, displaying each result's title. Finally, it outputs the generated answer to the user.  This function orchestrates a search and summarization pipeline, with the query as input and no explicit return value (void)."
  },
  {
    "type": "function",
    "name": "tf_command",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def tf_command(doc_id: int, term: str) -> int:\n    invertedIdx = InvertedIndex()\n    invertedIdx.load()\n    return invertedIdx.get_tf(doc_id,term)",
    "start_line": 176,
    "end_line": 179,
    "description": "The `tf_command` function calculates the term frequency (TF) of a given term within a specific document. It takes a document ID (`doc_id`) and a search term (`term`) as input.  It loads an inverted index data structure (`invertedIdx`), then utilizes this index to retrieve the term frequency. The function returns an integer representing the number of times the `term` appears in the document identified by `doc_id`."
  },
  {
    "type": "function",
    "name": "idf_command",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def idf_command(term: str) -> float:\n    invertedIdx = InvertedIndex()\n    invertedIdx.load()\n    return invertedIdx.get_idf(term)",
    "start_line": 181,
    "end_line": 184,
    "description": "The `idf_command` function calculates the Inverse Document Frequency (IDF) for a given term within a corpus of text.  It leverages an `InvertedIndex` object, which is assumed to be pre-loaded with document frequencies.  The function takes a `term` (string) as input. It loads the pre-built inverted index, then calls the `get_idf` method to retrieve the IDF value (a float) for the specified term. This IDF value, representing the term's document-level rarity, is then returned."
  },
  {
    "type": "function",
    "name": "bm25_tf_command",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def bm25_tf_command(doc_id: int, term: str, k1: float = BM25_K1, b: float = BM25_B) -> float:\n    invertedIdx = InvertedIndex()\n    invertedIdx.load()\n    return invertedIdx.get_bm25_tf(doc_id,term)",
    "start_line": 186,
    "end_line": 189,
    "description": "The `bm25_tf_command` function calculates the BM25 term frequency (TF) score for a given term within a specific document. It takes the document ID (`doc_id`) and the search term (`term`) as input, along with optional BM25 parameters `k1` and `b` (defaulting to global constants).  Internally, it utilizes an `InvertedIndex` object, which is loaded to facilitate the BM25 calculation. The function then uses the loaded index's `get_bm25_tf` method to perform the core BM25 calculation, returning the computed score as a float."
  },
  {
    "type": "function",
    "name": "bm25_idf_command",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def bm25_idf_command(term: str) -> float:\n    invertedIdx = InvertedIndex()\n    invertedIdx.load()\n    return invertedIdx.get_bm25_idf(term)",
    "start_line": 191,
    "end_line": 194,
    "description": "The `bm25_idf_command` function calculates the BM25 Inverse Document Frequency (IDF) score for a given `term`. It loads an existing inverted index using the `InvertedIndex` class. The core logic involves using the `get_bm25_idf` method of the `InvertedIndex` object, which applies the BM25 formula to compute the IDF. This function effectively retrieves the IDF component of the BM25 ranking for a search term, returning a float representing the term's inverse document frequency weight."
  },
  {
    "type": "function",
    "name": "bm25_search_command",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def bm25_search_command(query: str, limit: int=DEFAULT_SEARCH_LIMIT) -> list[dict]:\n    invertedIdx = InvertedIndex()\n    invertedIdx.load()\n    return invertedIdx.bm25_search(query, limit)",
    "start_line": 196,
    "end_line": 199,
    "description": "The `bm25_search_command` function performs a search based on the BM25 algorithm. It takes a search `query` string and an optional `limit` for the number of results as input.  It loads an inverted index using `InvertedIndex()`, then utilizes the index's `bm25_search` method to find relevant documents. Finally, it returns a list of dictionaries, likely containing search results, capped at the specified `limit`."
  },
  {
    "type": "function",
    "name": "search_command",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def search_command(query: str, limit: int=DEFAULT_SEARCH_LIMIT) -> list[dict]:\n    invertedIdx = InvertedIndex()\n    invertedIdx.load()\n    results = retrieve_documents(query, invertedIdx, limit)\n    return results",
    "start_line": 201,
    "end_line": 205,
    "description": "The `search_command` function performs a search based on a given query. It uses an inverted index to efficiently find relevant documents. It loads the inverted index, retrieves document results matching the query, and limits the results to a specified number.\n\n*   **Parameters:** `query` (search string), `limit` (maximum results, defaults to `DEFAULT_SEARCH_LIMIT`).\n*   **Return Value:** A list of dictionaries, where each dictionary represents a document and contains search results.\n*   **Algorithm:** Utilizes an inverted index for fast document retrieval."
  },
  {
    "type": "function",
    "name": "retrieve_documents",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def retrieve_documents(query: str, invertedIdx: InvertedIndex, limit: int) -> list[dict]:\n    query_tokenized = pre_process(query)\n    seen = set()\n    results = []\n    for query_token in query_tokenized: \n        doc_ids = invertedIdx.get_documents(query_token)\n        for doc_id in doc_ids:\n            if doc_id in seen:\n                continue\n            seen.add(doc_id)\n            doc = invertedIdx.docmap[doc_id]\n            if not doc:\n                continue\n            if len(results)<limit:\n                results.append(doc)\n            \n    return results",
    "start_line": 207,
    "end_line": 223,
    "description": "The `retrieve_documents` function searches for documents matching a given `query` within an `InvertedIndex`. It first preprocesses the query.  It then uses the inverted index to fetch document IDs associated with query tokens.  Duplicate document IDs are filtered. It retrieves up to a specified `limit` of unique documents, adding only those present in the inverted index.  The function returns a list of dictionaries, where each dictionary represents a document."
  },
  {
    "type": "function",
    "name": "build_command",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def build_command() -> int:\n    invertedIdx = InvertedIndex()\n    invertedIdx.build()\n    invertedIdx.save()",
    "start_line": 225,
    "end_line": 228,
    "description": "The `build_command` function constructs and persists an inverted index. It initializes an `InvertedIndex` object, then triggers its internal `build()` method to populate the index with data. Finally, it calls the `save()` method on the `InvertedIndex` object to store the index to disk. The function doesn't take any parameters and returns an integer representing the outcome of saving the index."
  },
  {
    "type": "function",
    "name": "pre_process",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def pre_process(input: str, isQuery=False) -> list:\n    return stem_words(remove_stopwords(tokenize(remove_punctuation(convert_to_lower(input)))),isQuery)",
    "start_line": 232,
    "end_line": 233,
    "description": "The `pre_process` function prepares text data for analysis. It takes a string `input` and a boolean `isQuery` as input.  It converts the input to lowercase, removes punctuation, tokenizes the text into words, removes stop words, and optionally stems the words (if `isQuery` is False). It returns a list of preprocessed words, effectively cleaning and normalizing the input text for tasks like information retrieval or natural language processing."
  },
  {
    "type": "function",
    "name": "convert_to_lower",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def convert_to_lower(input: str) -> str:\n    return input.lower()",
    "start_line": 235,
    "end_line": 236,
    "description": "The `convert_to_lower` function transforms a given string to lowercase. It takes a single string, `input`, as its parameter.  Internally, it utilizes the built-in `.lower()` string method to convert all characters within the input string to their lowercase equivalents. The function then returns the modified lowercase string. This operation provides a straightforward way to normalize text data, making comparisons and processing consistent regardless of the original capitalization."
  },
  {
    "type": "function",
    "name": "remove_punctuation",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def remove_punctuation(input: str) -> str:\n    return input.translate(str.maketrans('','', string.punctuation)) ",
    "start_line": 238,
    "end_line": 239,
    "description": "The `remove_punctuation` function efficiently cleans a string by removing all punctuation. It accepts a single string, `input`, as its argument.  The core logic leverages `str.maketrans` to create a translation table, mapping all punctuation characters to an empty string for deletion. The `translate` method then applies this table, returning a new string devoid of punctuation.  The function returns the punctuation-free string. This offers a clean and concise way to preprocess text for tasks like natural language processing."
  },
  {
    "type": "function",
    "name": "tokenize",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def tokenize(input: str)-> list[str]:\n    return input.strip().split()",
    "start_line": 241,
    "end_line": 242,
    "description": "The `tokenize` function converts a string into a list of individual words (tokens). It takes a string as input. First, it removes leading and trailing whitespace using `.strip()`. Then, it splits the string into a list of words, using whitespace as the delimiter, with `.split()`. The function returns a list of strings, where each string represents a word from the original input. This is a basic but fundamental text processing function."
  },
  {
    "type": "function",
    "name": "remove_stopwords",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def remove_stopwords(input: list[str]) -> list[str]:\n    stopwords = read_stopwords()\n    new_list = []\n    for word in input:\n        if word not in stopwords:\n            new_list.append(word)\n    return new_list",
    "start_line": 244,
    "end_line": 250,
    "description": "The `remove_stopwords` function filters a list of strings, eliminating common words (stopwords). It takes a list of strings (`input`) as input.  Internally, it utilizes a pre-defined set of stopwords obtained via the `read_stopwords()` function.  The function iterates through the input list, appending words *not* present in the stopwords set to a new list.  It returns this new list, effectively removing all stopwords from the original input. This process uses a straightforward filtering approach based on set membership."
  },
  {
    "type": "function",
    "name": "stem_words",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "def stem_words(input: list[str], isQuery=False)->list[str]:\n    stemmer = PorterStemmer()\n    stemmed_list = []\n    for token in input:\n        stemmed_word = stemmer.stem(token)\n        if isQuery and stemmed_word not in stemmed_list:\n            stemmed_list.append(stemmed_word)\n            continue\n        stemmed_list.append(stemmed_word)\n    return stemmed_list",
    "start_line": 253,
    "end_line": 262,
    "description": "The `stem_words` function processes a list of words, reducing each to its stem using the Porter stemming algorithm.  It takes a list of strings (`input`) as input and an optional boolean `isQuery`. The core logic iterates through the input, stemming each word.  If `isQuery` is True, it ensures that only unique stemmed words are added to the output list, useful for query optimization.  The function returns a new list containing the stemmed words."
  },
  {
    "type": "function",
    "name": "__init__",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def __init__(self) -> None:\n        self.index = defaultdict(set)\n        self.docmap: dict[int, dict] = {}\n        self.index_path = os.path.join(CACHE_DIR, \"index.pkl\")\n        self.docmap_path = os.path.join(CACHE_DIR, \"docmap.pkl\")\n        self.term_frequencies = defaultdict(Counter)\n        self.term_frequencies_path = os.path.join(CACHE_DIR, \"term_frequencies.pkl\")\n        self.doc_lengths: dict = {}\n        self.doc_lengths_path = os.path.join(CACHE_DIR, \"doc_lengths.pkl\")\n        self.avg_doc_length: float = 0.0\n        self.avg_doc_length_path = os.path.join(CACHE_DIR, \"avg_doc_length.pkl\")\n        self.avg_doc_length_path = os.path.join(CACHE_DIR, \"avg_doc_length.pkl\")",
    "start_line": 16,
    "end_line": 27,
    "description": "The `__init__` method initializes an object, likely for text indexing and retrieval. It sets up various data structures including a default dictionary for the index (`self.index`), a dictionary for document mappings (`self.docmap`), and dictionaries and paths for term frequencies (`self.term_frequencies`), document lengths (`self.doc_lengths`), and average document length (`self.avg_doc_length`).  It defines file paths for persistence within a cache directory.  No explicit parameters are taken, and the return value is `None`.  These initialized structures will likely store information used for search and ranking."
  },
  {
    "type": "function",
    "name": "__add_documents",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def __add_documents(self, doc_id: int, text: str) -> None:\n        tokenized_text = pre_process(text)\n        self.doc_lengths[doc_id] = len(tokenized_text)\n        for token in set(tokenized_text):\n            self.index[token].add(doc_id)\n        self.term_frequencies[doc_id].update(tokenized_text)",
    "start_line": 29,
    "end_line": 34,
    "description": "The `__add_documents` function indexes a text document. It takes a document ID (`doc_id`) and the document's text (`text`) as input.  It preprocesses the text, then updates three internal data structures. `doc_lengths` stores the document's token count. `index` maps unique tokens to a set of document IDs containing them.  `term_frequencies` tracks the frequency of each token within the document. The function uses `pre_process` to prepare the text, and leverages set operations and a Counter object (`term_frequencies`) for efficient indexing."
  },
  {
    "type": "function",
    "name": "__get_avg_doc_length",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def __get_avg_doc_length(self) -> float:\n        total = 0\n        for _, value in self.doc_lengths.items():\n            total+=value\n        return total/len(self.doc_lengths)",
    "start_line": 36,
    "end_line": 40,
    "description": "This function, `__get_avg_doc_length`, calculates the average document length within a collection of documents. It iterates through the `doc_lengths` dictionary (presumably storing document lengths), summing the individual lengths.  The function then divides the total length by the number of documents (obtained via `len(self.doc_lengths)`) to compute the average.  It returns the calculated average document length as a floating-point number."
  },
  {
    "type": "function",
    "name": "get_documents",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def get_documents(self, term: str) -> list[int]:\n        doc_ids = self.index.get(term.lower(), set())\n        return sorted(list(doc_ids))",
    "start_line": 42,
    "end_line": 44,
    "description": "The `get_documents` function retrieves document IDs associated with a given search term from an inverted index. It takes a `term` (string) as input and converts it to lowercase for case-insensitive matching.  It then accesses the index using the lowercase term to retrieve a set of document IDs.  Finally, it converts the set to a list, sorts it in ascending order, and returns the sorted list of document IDs.  If the term isn't found, an empty list is returned."
  },
  {
    "type": "function",
    "name": "get_tf",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def get_tf(self, doc_id: int, term: str) -> int:\n        tokenized_term = pre_process(term)\n        if len(tokenized_term) != 1:\n            raise Exception(\"Should be single token\")\n        tf = self.term_frequencies[doc_id][tokenized_term[0]]\n        return tf",
    "start_line": 48,
    "end_line": 53,
    "description": "The `get_tf` function retrieves the term frequency (TF) for a specific term within a given document. It takes a document ID (`doc_id`) and a term (`term`) as input.  The input term is preprocessed to create a single token.  It then accesses the pre-calculated term frequencies stored in `self.term_frequencies`, indexed by document ID and the tokenized term, and returns the frequency count as an integer. An exception is raised if the preprocessed term does not result in a single token."
  },
  {
    "type": "function",
    "name": "get_idf",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def get_idf(self, term: str) -> float:\n        tokenized_term = pre_process(term)\n        if len(tokenized_term) != 1:\n            raise Exception(\"Should be single token\")\n        doc_count = len(self.docmap)\n        term_doc_count = len(self.get_documents(tokenized_term[0]))\n        idf = math.log((doc_count + 1) / (term_doc_count + 1))\n        return idf",
    "start_line": 55,
    "end_line": 62,
    "description": "The `get_idf` function calculates the Inverse Document Frequency (IDF) for a given term within a document corpus. It first preprocesses the input `term` and validates it's a single token. It then counts the total number of documents and the number of documents containing the term.  Finally, it computes the IDF using the standard log-based formula, returning a float representing the term's IDF score, which is used for weighting terms in information retrieval tasks."
  },
  {
    "type": "function",
    "name": "get_bm25_idf",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def get_bm25_idf(self, term: str) -> float:\n        tokenized_term = pre_process(term)\n        if len(tokenized_term) != 1:\n            raise Exception(\"Should be single token\")\n        N = len(self.docmap)\n        df = len(self.get_documents(tokenized_term[0]))\n        bm_25 = math.log((N - df + 0.5) / (df + 0.5) + 1)\n        return max(bm_25, 0.0)\n        return max(bm_25, 0.0)",
    "start_line": 69,
    "end_line": 77,
    "description": "The `get_bm25_idf` function calculates the Inverse Document Frequency (IDF) weight for a given term using the BM25 formula.  It accepts a single term (string) and returns a float representing its IDF score.  It preprocesses the term, ensuring it's a single token.  Crucially, it determines the document frequency (`df`) of the term within a corpus of documents, using `self.get_documents()`.  The IDF is computed using the BM25 formula, incorporating the total number of documents (`N`).  The function ensures a non-negative IDF value by returning the maximum of the calculated score and 0.0."
  },
  {
    "type": "function",
    "name": "get_bm25_tf",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def get_bm25_tf(self, doc_id: int, term: str, k1: float = BM25_K1, b: float = BM25_B) -> float: \n        tf = self.get_tf(doc_id, term)\n        movie = self.docmap[doc_id]\n        doc_length = self.doc_lengths[doc_id]\n        doc_length = self.doc_lengths[doc_id]\n        avg_doc_length = self.avg_doc_length\n        length_norm = 1 - b + b * (doc_length / avg_doc_length)\n        tf_component = (tf * (k1 + 1)) / (tf + k1 * length_norm)\n        return tf_component",
    "start_line": 79,
    "end_line": 87,
    "description": "The `get_bm25_tf` function computes the term frequency component of the BM25 ranking algorithm for a given term within a document. It calculates this component based on the term frequency (`tf`), document length, average document length, and two BM25 parameters, `k1` and `b`. The function retrieves `tf` using `get_tf`, and uses document metadata stored in `docmap` and `doc_lengths`.  It incorporates document length normalization to mitigate bias towards longer documents, ultimately returning the computed term frequency component."
  },
  {
    "type": "function",
    "name": "bm25",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def bm25(self, doc_id: int, term: str) -> float:\n        bm25_tf = self.get_bm25_tf(doc_id, term)\n        bm25_idf = self.get_bm25_idf(term)\n        return bm25_tf*bm25_idf",
    "start_line": 89,
    "end_line": 92,
    "description": "The `bm25` function calculates the BM25 score for a given term within a document. It takes a document ID (`doc_id`) and the search term (`term`) as input.  It computes the term frequency component using `get_bm25_tf` and the inverse document frequency component using `get_bm25_idf`.  Finally, it returns the product of these two components, representing the overall relevance score according to the BM25 ranking algorithm, a standard technique for information retrieval."
  },
  {
    "type": "function",
    "name": "bm25_search",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def bm25_search(self, query: str, limit: int = DEFAULT_SEARCH_LIMIT) -> list[dict]:\n        \n        tokens = pre_process(query)\n        logging.info(f\"Query After enhancements: {tokens}\")\n\n        scores = defaultdict(float)\n\n        # using eligible movies to speed up the search\n        eligible_movies = retrieve_documents(query, self, len(self.docmap)).copy()\n        for movie in eligible_movies:\n            doc_id = movie['id']\n            score = 0.0\n            for token in tokens:\n                score += self.bm25(doc_id,token)\n            scores[doc_id] = score\n\n        # assigning 0 to the rest.\n        for doc_id, _ in self.docmap.items():\n            if doc_id not in scores:\n                scores[doc_id] = 0.0\n\n        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n\n        results = []\n        for doc_id, score in sorted_docs[:limit]:\n            document = self.docmap[doc_id].copy()\n            document[\"score\"] = score\n            results.append(document)\n\n        return results",
    "start_line": 96,
    "end_line": 125,
    "description": "The `bm25_search` function implements a BM25 search algorithm for a movie database.  It takes a user query string and an optional `limit` for the number of results. It preprocesses the query, retrieves a subset of eligible documents, and then calculates a BM25 score for each document based on the query tokens. The function aggregates scores, sorts documents by score (highest first), and returns a list of the top-`limit` movies, each augmented with its search score.  It uses `pre_process`, `retrieve_documents`, and an internal `bm25` method for core search operations."
  },
  {
    "type": "function",
    "name": "build",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def build(self) -> None:\n        movies = load_movies()\n        for movie in movies:\n            doc_id = movie['id']\n            self.docmap[doc_id]=movie\n            self.__add_documents(doc_id,f\"{movie['title']} {movie['description']}\")\n        self.avg_doc_length = self.__get_avg_doc_length()\n        self.avg_doc_length = self.__get_avg_doc_length()",
    "start_line": 127,
    "end_line": 134,
    "description": "The `build` function constructs an in-memory document index. It loads a list of movie data using `load_movies()`. For each movie, it stores the movie data in a dictionary `docmap` indexed by the movie's ID. It then indexes the movie's title and description using a private method `__add_documents`. Finally, it calculates and stores the average document length using a private method `__get_avg_doc_length`, calling it twice. This process effectively builds a searchable index from the movie data. The function returns `None`."
  },
  {
    "type": "function",
    "name": "build_from_documents",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def build_from_documents(self, documents: list[dict]) -> None:\n        \"\"\"Build index from a list of documents instead of loading movies.\"\"\"\n        for doc in documents:\n            doc_id = doc['id']\n            self.docmap[doc_id] = doc\n            # Combine title and description for indexing\n            text = f\"{doc.get('title', '')} {doc.get('description', '')}\"\n            self.__add_documents(doc_id, text)\n        self.avg_doc_length = self.__get_avg_doc_length()",
    "start_line": 136,
    "end_line": 144,
    "description": "The `build_from_documents` function populates an index from a list of document dictionaries. It iterates through the input `documents`, using each document's 'id' as a key. It stores the document itself in a `docmap`. The function then combines the 'title' and 'description' fields for indexing via the internal `__add_documents` method. Finally, it calculates and stores the average document length using `__get_avg_doc_length()`.  The function doesn't return a value."
  },
  {
    "type": "function",
    "name": "save",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def save(self) -> None:\n        os.makedirs(CACHE_DIR, exist_ok=True)\n        with open(self.index_path, \"wb\") as file:\n            pickle.dump(self.index, file)\n        with open(self.docmap_path, \"wb\") as file:\n            pickle.dump(self.docmap, file)\n        with open(self.term_frequencies_path, \"wb\") as file:\n            pickle.dump(self.term_frequencies, file)\n        with open(self.doc_lengths_path, \"wb\") as file:\n            pickle.dump(self.doc_lengths, file)\n        with open(self.avg_doc_length_path, \"wb\") as file:\n            pickle.dump(self.avg_doc_length, file)\n        \n        with open(self.avg_doc_length_path, \"wb\") as file:\n            pickle.dump(self.avg_doc_length, file)",
    "start_line": 146,
    "end_line": 160,
    "description": "The `save` function persists the state of a search index to disk. It uses the `pickle` module to serialize several key data structures: the inverted index (`self.index`), document map (`self.docmap`), term frequencies (`self.term_frequencies`), document lengths (`self.doc_lengths`), and average document length (`self.avg_doc_length`).  It first ensures the cache directory exists and then writes each object to a separate file within the cache using binary write mode (\"wb\").  The function takes no parameters and returns nothing (None)."
  },
  {
    "type": "function",
    "name": "load",
    "filepath": "cli/lib/keyword_search.py",
    "filename": "keyword_search.py",
    "content": "    def load(self):\n        with open(self.index_path, \"rb\") as file:\n            self.index = pickle.load(file)\n        with open(self.docmap_path, \"rb\") as file:\n            self.docmap = pickle.load(file)\n        with open(self.term_frequencies_path, \"rb\") as file:\n            self.term_frequencies = pickle.load(file)\n        with open(self.doc_lengths_path, \"rb\") as file:\n            self.doc_lengths = pickle.load(file)\n        with open(self.avg_doc_length_path, \"rb\") as file:\n            self.avg_doc_length = pickle.load(file)\n        with open(self.avg_doc_length_path, \"rb\") as file:\n            self.avg_doc_length = pickle.load(file)",
    "start_line": 162,
    "end_line": 174,
    "description": "The `load` function restores previously saved data structures from disk. It opens several files specified by class attributes ending with `_path`, each assumed to contain pickled data. These files hold the index, document map (`docmap`), term frequencies, document lengths, and the average document length. The function uses `pickle.load` to deserialize the data from each file and stores the results in corresponding instance variables (e.g., `self.index`, `self.docmap`). Essentially, it reloads the search index and related data for later use."
  },
  {
    "type": "function",
    "name": "__init__",
    "filepath": "cli/lib/vector_store.py",
    "filename": "vector_store.py",
    "content": "    def __init__(self, dimension: int, index_path: str, metadata_path: str) -> None:\n        self.dimension = dimension\n        self.index_path = index_path\n        self.metadata_path = metadata_path\n        self.index: faiss.Index | None = None\n        self.metadata: Dict | None = None",
    "start_line": 18,
    "end_line": 23,
    "description": "This Python function, `__init__`, is the constructor for a class, likely related to vector indexing. It initializes key attributes of an object: `dimension`, which specifies the vector dimensionality; `index_path` and `metadata_path`, which store file paths for the index and associated metadata, respectively.  It also initializes `index` and `metadata` to `None`, preparing them for later population. This sets up the object's fundamental state for subsequent operations like building or querying an index."
  },
  {
    "type": "function",
    "name": "is_ready",
    "filepath": "cli/lib/vector_store.py",
    "filename": "vector_store.py",
    "content": "    def is_ready(self) -> bool:\n        return os.path.exists(self.index_path) and os.path.exists(self.metadata_path)",
    "start_line": 25,
    "end_line": 26,
    "description": "The `is_ready` function checks if the system's index and metadata files are present. It uses the `os.path.exists()` function to verify the existence of the files located at `self.index_path` and `self.metadata_path`. The function returns `True` if both files exist, indicating the system is ready, and `False` otherwise. This provides a quick check for the necessary components before performing other operations."
  },
  {
    "type": "function",
    "name": "count",
    "filepath": "cli/lib/vector_store.py",
    "filename": "vector_store.py",
    "content": "    def count(self) -> int:\n        if self.index is None:\n            return 0\n        return self.index.ntotal",
    "start_line": 28,
    "end_line": 31,
    "description": "The `count` function efficiently determines the total number of items associated with a data structure. It checks if an internal index (`self.index`) is initialized. If the index is not initialized (is `None`), the function returns 0. Otherwise, it retrieves and returns the total count directly from the index using `self.index.ntotal`, offering a quick count lookup without iterating through the data. This leverages a pre-computed count for performance."
  },
  {
    "type": "function",
    "name": "_normalize",
    "filepath": "cli/lib/vector_store.py",
    "filename": "vector_store.py",
    "content": "    def _normalize(self, embeddings: np.ndarray) -> np.ndarray:\n        if embeddings.ndim == 1:\n            embeddings = embeddings.reshape(1, -1)\n        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n        norms[norms == 0] = 1.0\n        return (embeddings / norms).astype(np.float32)",
    "start_line": 33,
    "end_line": 38,
    "description": "The `_normalize` function normalizes a NumPy array of embeddings.  It handles both single and multi-dimensional embedding inputs. It calculates the L2 norm (Euclidean norm) of each embedding vector along the specified axis. It then divides each embedding by its norm to produce a normalized vector. A check prevents division by zero by replacing zero norms with 1.  The function returns the normalized embeddings as a float32 NumPy array."
  },
  {
    "type": "function",
    "name": "_ensure_parent_dir",
    "filepath": "cli/lib/vector_store.py",
    "filename": "vector_store.py",
    "content": "    def _ensure_parent_dir(self) -> None:\n        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)",
    "start_line": 40,
    "end_line": 41,
    "description": "The `_ensure_parent_dir` function ensures the existence of the parent directory for a given index file path. It takes the `index_path` attribute of the class instance (presumably a file path string) and extracts its parent directory using `os.path.dirname`.  `os.makedirs` is then called to create the parent directory and any missing intermediate directories, if they do not exist. The `exist_ok=True` argument prevents an error if the parent directory already exists, making the function idempotent.  The function doesn't return anything (returns `None`)."
  },
  {
    "type": "function",
    "name": "rebuild",
    "filepath": "cli/lib/vector_store.py",
    "filename": "vector_store.py",
    "content": "    def rebuild(self, embeddings: np.ndarray, chunk_metadata: List[Dict]) -> None:\n        if embeddings.size == 0 or len(chunk_metadata) == 0:\n            raise ValueError(\"No chunk embeddings were provided to build the store.\")\n\n        normalized = self._normalize(embeddings)\n        if normalized.shape[1] != self.dimension:\n            raise ValueError(\n                f\"Embedding dimension mismatch. Expected {self.dimension}, got {normalized.shape[1]}\"\n            )\n\n        self.index = faiss.IndexFlatIP(self.dimension)\n        self.index.add(normalized)\n        self.metadata = {\n            \"chunks\": chunk_metadata,\n            \"total_chunks\": len(chunk_metadata),\n        }\n\n        self._ensure_parent_dir()\n        faiss.write_index(self.index, self.index_path)\n        with open(self.metadata_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.metadata, f, indent=2)",
    "start_line": 43,
    "end_line": 63,
    "description": "The `rebuild` function reconstructs a vector store with new embeddings and associated metadata. It takes a NumPy array of embeddings and a list of chunk metadata dictionaries as input.  It normalizes the embeddings, validates their dimensionality, and then builds a Faiss index (IndexFlatIP) using the normalized data.  The function adds the embeddings to the index, stores the chunk metadata and total chunk count, and finally, persists the index and metadata to disk using Faiss and JSON, respectively. It handles potential errors like empty input or dimension mismatches."
  },
  {
    "type": "function",
    "name": "load",
    "filepath": "cli/lib/vector_store.py",
    "filename": "vector_store.py",
    "content": "    def load(self) -> None:\n        if os.path.exists(self.index_path):\n            self.index = faiss.read_index(self.index_path)\n        else:\n            self.index = None\n\n        if os.path.exists(self.metadata_path):\n            with open(self.metadata_path, \"r\", encoding=\"utf-8\") as f:\n                self.metadata = json.load(f)\n        else:\n            self.metadata = None",
    "start_line": 65,
    "end_line": 75,
    "description": "The `load` function initializes a search index and associated metadata from disk. It attempts to load a pre-built FAISS index from `self.index_path` using `faiss.read_index()`. If the index file doesn't exist, `self.index` is set to `None`.  Similarly, it loads metadata from `self.metadata_path` as a JSON file, setting `self.metadata` to `None` if the file is missing. This function facilitates resuming search operations from a previously saved state."
  },
  {
    "type": "function",
    "name": "ensure_loaded",
    "filepath": "cli/lib/vector_store.py",
    "filename": "vector_store.py",
    "content": "    def ensure_loaded(self) -> None:\n        if self.index is None or self.metadata is None:\n            self.load()",
    "start_line": 77,
    "end_line": 79,
    "description": "The `ensure_loaded` function guarantees that a data object's index and metadata are loaded. It checks if the `index` or `metadata` attributes are currently `None`. If either is `None`, indicating that the data is not loaded, it calls the internal `load()` function to initialize them. This ensures the data object is ready for use, potentially avoiding errors from accessing unloaded data. The function takes no parameters and returns `None`."
  },
  {
    "type": "function",
    "name": "search",
    "filepath": "cli/lib/vector_store.py",
    "filename": "vector_store.py",
    "content": "    def search(self, query_embedding: np.ndarray, top_k: int) -> List[Tuple[Dict, float]]:\n        self.ensure_loaded()\n        if self.index is None or self.metadata is None:\n            return []\n\n        normalized_query = self._normalize(query_embedding)\n        limit = min(top_k, self.index.ntotal)\n        if limit <= 0:\n            return []\n        scores, indices = self.index.search(normalized_query, limit)\n\n        results: List[Tuple[Dict, float]] = []\n        for score, idx in zip(scores[0], indices[0]):\n            if idx == -1:\n                continue\n            chunk_metadata = self.metadata[\"chunks\"][int(idx)]\n            results.append((chunk_metadata, float(score)))\n        return results",
    "start_line": 81,
    "end_line": 98,
    "description": "The `search` function performs a similarity search within a pre-built index. It takes a query embedding and `top_k` as input, returning the `top_k` most similar metadata entries. It normalizes the query, searches the index using the query and `top_k`, retrieves associated metadata based on the index results, and returns a list of tuples containing the metadata and similarity scores. The function handles edge cases where the index is not loaded or when `top_k` is invalid, returning an empty list in such situations."
  },
  {
    "type": "function",
    "name": "read_img",
    "filepath": "cli/lib/describe_image.py",
    "filename": "describe_image.py",
    "content": "def read_img(image):\n    with open(image, \"rb\") as f:\n        img = f.read()\n        return img",
    "start_line": 15,
    "end_line": 18,
    "description": "The `read_img` function efficiently reads an image file's binary data. It takes the image's file path as a string (`image`) as input. Utilizing a `with open()` statement ensures proper file handling, opening the file in binary read mode (\"rb\").  The entire content of the file is read into a variable `img`. The function then returns `img`, the binary data of the image, allowing for further processing or manipulation of the image data."
  },
  {
    "type": "function",
    "name": "describe_image",
    "filepath": "cli/lib/describe_image.py",
    "filename": "describe_image.py",
    "content": "def describe_image(query: str, image: str): \n    \n    mime, _ = mimetypes.guess_type(image)\n    mime = mime or \"image/jpeg\"\n\n    img = read_img(image)\n\n    system_prompt = f\"\"\"\n    Given the included image and text query, rewrite the text query to improve search results from a    movie database. Make sure to:\n    - Synthesize visual and textual information\n    - Focus on movie-specific details (actors, scenes, style, etc.)\n    - Return only the rewritten query, without any additional commentary\n    \"\"\"\n    \n    parts = [\n    system_prompt,\n    genai.types.Part.from_bytes(data=img, mime_type=mime),\n    query.strip(),\n    ]\n\n    response = client.models.generate_content(model=model, contents=parts)\n\n    return response",
    "start_line": 20,
    "end_line": 42,
    "description": "The `describe_image` function refines a text query for a movie database search using an image and a user-provided query.  It reads the image, determines its MIME type, and constructs a prompt combining a system instruction, the image data, and the original query. The function leverages a generative AI model to rewrite the query, integrating visual and textual information to focus on movie-specific details. The returned value is the rewritten query string."
  },
  {
    "type": "function",
    "name": "describe_image_command",
    "filepath": "cli/lib/describe_image.py",
    "filename": "describe_image.py",
    "content": "def describe_image_command(query: str, image: str) -> None:\n    image_path = os.path.join(PROJECT_ROOT, image)\n    response = describe_image(query, image_path)\n\n    print(f\"Rewritten query: {response.text.strip()}\")\n    if response.usage_metadata is not None:\n        print(f\"Total tokens:    {response.usage_metadata.total_token_count}\")",
    "start_line": 46,
    "end_line": 52,
    "description": "The `describe_image_command` function processes an image based on a user-provided query. It takes a `query` string and the `image`'s relative path as input. It utilizes an internal `describe_image` function (not defined here) to analyze the image using the query. The function then prints the rewritten query (obtained from the analysis) to the console. If usage metadata is available, it also displays the total token count."
  },
  {
    "type": "function",
    "name": "spell_correct",
    "filepath": "cli/lib/query_enhancement.py",
    "filename": "query_enhancement.py",
    "content": "def spell_correct(query: str) -> str:\n    prompt = f\"\"\"Fix any spelling errors in this movie search query.\n\nOnly correct obvious typos. Don't change correctly spelled words.\n\nQuery: \"{query}\"\n\nIf no errors, return the original query.\nCorrected:\"\"\"\n\n    response = client.models.generate_content(model=model, contents=prompt)\n    corrected = (response.text or \"\").strip().strip('\"')\n    return corrected if corrected else query",
    "start_line": 13,
    "end_line": 25,
    "description": "The `spell_correct` function enhances movie search queries by correcting spelling errors. It takes a search query string as input. It utilizes a pre-trained model (accessed through a client) to generate a corrected version of the query.  The function only attempts to fix obvious typos, preserving the original query if no errors are detected. It returns the corrected query string or the original query if no correction is made, ensuring query stability when no spelling errors exist."
  },
  {
    "type": "function",
    "name": "rewrite_query",
    "filepath": "cli/lib/query_enhancement.py",
    "filename": "query_enhancement.py",
    "content": "def rewrite_query(query: str) -> str:\n    prompt = f\"\"\"Rewrite this movie search query to be more specific and searchable.\n\nOriginal: \"{query}\"\n\nConsider:\n- Common movie knowledge (famous actors, popular films)\n- Genre conventions (horror = scary, animation = cartoon)\n- Keep it concise (under 10 words)\n- It should be a google style search query that's very specific\n- Don't use boolean logic\n\nExamples:\n\n- \"that bear movie where leo gets attacked\" -> \"The Revenant Leonardo DiCaprio bear attack\"\n- \"movie about bear in london with marmalade\" -> \"Paddington London marmalade\"\n- \"scary movie with bear from few years ago\" -> \"bear horror movie 2015-2020\"\n\nRewritten query:\"\"\"\n\n    response = client.models.generate_content(model=model, contents=prompt)\n    rewritten = (response.text or \"\").strip().strip('\"')\n    return rewritten if rewritten else query",
    "start_line": 28,
    "end_line": 50,
    "description": "The `rewrite_query` function refines movie search queries for improved results. It takes a user's `query` as input, constructing a prompt for a language model to rewrite it, aiming for specificity and conciseness (under 10 words).  The rewritten query leverages common movie knowledge and genre conventions. The function uses a language model via `client.models.generate_content` and returns the rewritten query or the original query if rewriting fails. It returns a Google-style search query, removing unnecessary quotes and white space."
  },
  {
    "type": "function",
    "name": "expand_query",
    "filepath": "cli/lib/query_enhancement.py",
    "filename": "query_enhancement.py",
    "content": "def expand_query(query: str) -> str:\n    prompt = f\"\"\"Expand this movie search query with related terms.\n\nAdd synonyms and related concepts that might appear in movie descriptions.\nKeep expansions relevant and focused.\nThis will be appended to the original query. \nONLY return the expanded query text without any markdowns.\n\nExamples:\n\n- query: \"scary bear movie\" expanded: \"scary horror grizzly bear movie terrifying film\"\n- query: \"action movie with bear\" expanded:  \"action thriller bear chase fight adventure\"\n- query: \"comedy with bear\" expanded: \"comedy funny bear humor lighthearted\"\n\nQuery: \"{query}\"\n\"\"\"\n\n    response = client.models.generate_content(model=model, contents=prompt)\n    expanded_terms = (response.text or \"\").strip().strip('\"')\n\n    return f\"{query} {expanded_terms}\"",
    "start_line": 53,
    "end_line": 73,
    "description": "The `expand_query` function enhances a movie search query using a language model. It takes a `query` string as input and uses a prompt to instruct the model to generate related terms, including synonyms and related concepts. The expanded terms are appended to the original query. The function leverages a language model (identified by `client.models.generate_content`) to generate the expansion, returning the combined original and expanded query as a single string. It focuses on relevant additions, preventing irrelevant terms."
  },
  {
    "type": "function",
    "name": "enhance_query",
    "filepath": "cli/lib/query_enhancement.py",
    "filename": "query_enhancement.py",
    "content": "def enhance_query(query: str, method: Optional[str] = None) -> str:\n    match method:\n        case \"spell\":\n            return spell_correct(query)\n        case \"rewrite\":\n            return rewrite_query(query)\n        case \"expand\":\n            return expand_query(query)\n        case _:\n            return query",
    "start_line": 75,
    "end_line": 84,
    "description": "The `enhance_query` function refines a given search query string based on a specified method. It accepts the original query (`query: str`) and an optional enhancement method (`method: Optional[str]`).  If a method (\"spell\", \"rewrite\", or \"expand\") is provided, the function calls a corresponding helper function (e.g., `spell_correct`) to modify the query.  If no method is specified, or an unrecognized method is provided, the original query is returned unchanged. The function returns the enhanced (or original) query string."
  },
  {
    "type": "function",
    "name": "rrf_score",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "def rrf_score(rank, k: int = DEFAULT_K_VALUE):\n    \"\"\"Calculate RRF score for a given rank.\"\"\"\n    return 1 / (k + rank)",
    "start_line": 22,
    "end_line": 24,
    "description": "The `rrf_score` function computes the Reciprocal Rank Fusion (RRF) score for a given document rank. It takes the document's `rank` (an integer representing its position in a ranked list) as input, and optionally, an integer `k` (defaulting to `DEFAULT_K_VALUE`). The function calculates the RRF score using the formula `1 / (k + rank)`. This score is commonly used in information retrieval and meta-search to combine rankings from multiple sources, favoring higher-ranked documents."
  },
  {
    "type": "function",
    "name": "rewrite_query",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "def rewrite_query(query: str, api_key: str = None) -> str:\n    \"\"\"\n    Rewrites a user query to be more suitable for searching function descriptions.\n    Focus on expanding the query with related concepts and technical terms.\n    \"\"\"\n    if not api_key:\n        api_key = os.getenv(\"GEMINI_API_KEY\")\n    \n    if not api_key:\n        return query # Fallback to original query if no key\n        \n    try:\n        client = genai.Client(api_key=api_key)\n        prompt = f\"\"\"You are a search query expert. Your task is to rewrite user questions into comprehensive search queries that will match against technical function descriptions.\n\nThe search system uses function descriptions (not code or function names), so focus on:\n- Key concepts and technical terms related to the question\n- Related operations, processes, or workflows\n- Alternative phrasings and synonyms\n- Domain-specific terminology\n\nExamples:\n\nUser: \"How do I log in?\"\nRewritten: \"user authentication login session management credential verification password check login process user authorization\"\n\nUser: \"Where is the database connection handled?\"\nRewritten: \"database connection management session creation database initialization connection pooling database setup establish connection\"\n\nUser: \"Show me the code for searching movies\"\nRewritten: \"movie search functionality search implementation query processing search results retrieval movie lookup find movies\"\n\nUser: \"How are images processed?\"\nRewritten: \"image processing image manipulation encoding decoding image transformation image handling base64 conversion\"\n\nUser: \"{query}\"\nRewritten:\"\"\"\n        \n        response = client.models.generate_content(model=\"gemini-2.0-flash\", contents=prompt)\n        rewritten = response.text.strip().replace('\"', '')\n        return rewritten\n    except Exception as e:\n        print(f\"Error rewriting query: {e}\")\n        return query",
    "start_line": 26,
    "end_line": 69,
    "description": "The `rewrite_query` function enhances user search queries for technical function descriptions. It uses a Gemini API call (requiring an API key) to expand the original query with related concepts, technical terms, and alternative phrasings. The expanded query aims to improve the search's ability to match against relevant function descriptions. If an API key is unavailable or the API call fails, the function defaults to returning the original query."
  },
  {
    "type": "function",
    "name": "build_codebase_index_command",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "def build_codebase_index_command():\n    rag = CodebaseRAG()\n    rag.build_index()",
    "start_line": 463,
    "end_line": 465,
    "description": "The `build_codebase_index_command` function constructs an index for a codebase, facilitating efficient retrieval of code-related information.  It initializes a `CodebaseRAG` object, presumably responsible for managing the indexing process.  The core functionality lies within the `rag.build_index()` call, which generates the index using an unspecified algorithm within the `CodebaseRAG` class.  The function itself takes no parameters and doesn't explicitly return a value, instead modifying the state of the `CodebaseRAG` object by building the index."
  },
  {
    "type": "function",
    "name": "search_codebase_command",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "def search_codebase_command(query: str, limit: int = 5):\n    rag = CodebaseRAG()\n    results = rag.search(query, limit)\n    for i, res in enumerate(results, 1):\n        print(f\"{i}. {res['filename']}:{res['name']} (Score: {res['score']:.4f})\")\n        print(f\"   {res['filepath']}\")",
    "start_line": 467,
    "end_line": 472,
    "description": "The `search_codebase_command` function searches a codebase for code snippets matching a given `query`. It utilizes a `CodebaseRAG` object (assumed to be a Retrieval-Augmented Generation system).  The function takes a `query` string and an optional `limit` (defaulting to 5) specifying the maximum number of results. It calls the `search` method of the `CodebaseRAG` object.  The results, each containing filename, function/class name, file path and score, are then printed to the console in a user-friendly numbered format, including the file path and score."
  },
  {
    "type": "function",
    "name": "__init__",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def __init__(self, root_dir: str, api_key: str = None):\n        self.root_dir = root_dir\n        self.ignore_patterns = self._load_gitignore()\n        self.api_key = api_key or os.getenv(\"GEMINI_API_KEY\")\n        self._rate_limit_hit = False  # Circuit breaker flag\n        self.cached_descriptions = self._load_cached_descriptions()\n        \n        if self.api_key:\n            self.client = genai.Client(api_key=self.api_key)\n        else:\n            self.client = None\n            print(\"Warning: No Gemini API key found. AI descriptions will be disabled.\")",
    "start_line": 72,
    "end_line": 83,
    "description": "The `__init__` function initializes an object likely designed for semantic search or information retrieval. It loads a SentenceTransformer model (`all-MiniLM-L6-v2`) for generating sentence embeddings and a CrossEncoder model (`cross-encoder/ms-marco-MiniLM-L-6-v2`) for reranking results.  It also initializes instance variables to store text chunks (`self.chunks`), their embeddings (`self.embeddings`), and potentially a keyword index (`self.keyword_index`).  These initialized attributes are crucial for processing text and retrieving relevant information."
  },
  {
    "type": "function",
    "name": "_load_cached_descriptions",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def _load_cached_descriptions(self) -> Dict[str, str]:\n        \"\"\"Load existing descriptions from cache/codebase_data.json to avoid re-generation.\"\"\"\n        cache_path = os.path.join(CACHE_DIR, \"codebase_data.json\")\n        descriptions = {}\n        if os.path.exists(cache_path):\n            try:\n                with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n                    data = json.load(f)\n                    for item in data:\n                        # Key by filename:function_name\n                        key = f\"{item['filename']}:{item['name']}\"\n                        descriptions[key] = item['description']\n                print(f\"Loaded {len(descriptions)} cached descriptions.\")\n            except Exception as e:\n                print(f\"Error loading cached descriptions: {e}\")\n        return descriptions",
    "start_line": 85,
    "end_line": 100,
    "description": "The `_load_cached_descriptions` function retrieves previously generated code descriptions from a JSON cache file, \"codebase_data.json\".  It attempts to read the file located within a specified cache directory. The function parses the JSON data, creates a dictionary where the keys are file:function names (e.g., \"my_file.py:my_function\"), and the values are the corresponding descriptions. If the cache file exists and can be read successfully, it returns the dictionary of loaded descriptions. Otherwise, it returns an empty dictionary, handling potential file reading errors gracefully."
  },
  {
    "type": "function",
    "name": "_load_gitignore",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def _load_gitignore(self) -> List[str]:\n        gitignore_path = os.path.join(self.root_dir, \".gitignore\")\n        patterns = []\n        if os.path.exists(gitignore_path):\n            with open(gitignore_path, \"r\") as f:\n                for line in f:\n                    line = line.strip()\n                    if line and not line.startswith(\"#\"):\n                        patterns.append(line)\n        # Add default ignores\n        patterns.extend([\".git\", \"__pycache__\", \"*.pyc\", \".DS_Store\", \".venv\", \".env\"])\n        return patterns",
    "start_line": 102,
    "end_line": 113,
    "description": "The `_load_gitignore` function retrieves ignore patterns from a project's `.gitignore` file. It reads each line, discarding comments and empty lines, and appends the valid patterns to a list. Key parameters include the implicit `self.root_dir`, representing the project's root directory. It then extends the list with a set of default ignore patterns. The function returns a list of strings, each string representing a file or directory pattern to be ignored during operations like code analysis or file comparisons."
  },
  {
    "type": "function",
    "name": "_is_ignored",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def _is_ignored(self, path: str) -> bool:\n        rel_path = os.path.relpath(path, self.root_dir)\n        filename = os.path.basename(path)\n        \n        # Explicitly ignore admin_panel_ui.py\n        if filename == \"admin_panel_ui.py\":\n            return True\n            \n        for pattern in self.ignore_patterns:\n            if fnmatch.fnmatch(rel_path, pattern) or fnmatch.fnmatch(filename, pattern):\n                return True\n            # Handle directory matching\n            if os.path.isdir(path) and fnmatch.fnmatch(rel_path + \"/\", pattern):\n                return True\n        return False",
    "start_line": 115,
    "end_line": 129,
    "description": "The `_is_ignored` function determines if a given file path should be ignored based on pre-defined patterns. It takes a `path` (string) as input. It first checks for an explicit ignore condition (`admin_panel_ui.py`). Then, it compares the relative path and filename against a list of `ignore_patterns` using `fnmatch`. If any pattern matches either the relative path, filename, or the relative path plus a trailing slash (for directories), the function returns `True`, indicating the file should be ignored. Otherwise, it returns `False`."
  },
  {
    "type": "function",
    "name": "generate_description",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def generate_description(self, code: str, function_name: str) -> str:\n        \"\"\"Generate an AI description for a function using Gemini with retry logic.\"\"\"\n        # Check circuit breaker\n        if self._rate_limit_hit:\n            return f\"Function {function_name}\"\n            \n        if not self.client:\n            return f\"Function {function_name}\"\n        \n        max_retries = 5\n        base_delay = 2  # Start with 2 seconds\n        \n        for attempt in range(max_retries):\n            try:\n                prompt = f\"\"\"You are a technical documentation expert. Write a concise 50-100 word description of what this Python function does. Focus on:\n- What the function accomplishes\n- Key parameters and return values\n- Important logic or algorithms used\n\nFunction name: {function_name}\n\nCode:\n```python\n{code}\n```\n\nDescription:\"\"\"\n                \n                response = self.client.models.generate_content(model=\"gemini-2.0-flash-lite\", contents=prompt)\n                description = response.text.strip()\n                \n                # Add a small delay between successful requests to avoid hitting rate limits\n                time.sleep(0.5)\n                \n                return description\n                \n            except Exception as e:\n                error_str = str(e)\n                \n                # Check if it's a rate limit error\n                if \"429\" in error_str or \"RESOURCE_EXHAUSTED\" in error_str:\n                    if attempt < max_retries - 1:\n                        # Exponential backoff: 2, 4, 8, 16, 32 seconds\n                        delay = base_delay * (2 ** attempt)\n                        print(f\"Rate limit hit for {function_name}, retrying in {delay}s... (attempt {attempt + 1}/{max_retries})\")\n                        time.sleep(delay)\n                        continue\n                    else:\n                        print(f\"Max retries reached for {function_name}. Disabling AI descriptions for this session.\")\n                        self._rate_limit_hit = True  # Trip circuit breaker\n                        return f\"Function {function_name}\"\n                else:\n                    # Non-rate-limit error, fail immediately\n                    print(f\"Error generating description for {function_name}: {e}\")\n                    return f\"Function {function_name}\"\n        \n        return f\"Function {function_name}\"",
    "start_line": 131,
    "end_line": 187,
    "description": "The `generate_description` function uses a Gemini AI model to create concise descriptions for Python functions. It takes the function's code and name as input. It constructs a prompt for the AI, sends it to the Gemini model, and retrieves the generated description. Crucially, it incorporates retry logic with exponential backoff to handle potential rate limits or other API errors. If rate limits are consistently hit, a circuit breaker is triggered to prevent future calls, returning a placeholder description. The function prioritizes successful retrieval but gracefully handles and recovers from errors when possible."
  },
  {
    "type": "function",
    "name": "chunk_file",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def chunk_file(self, filepath: str) -> List[Dict[str, Any]]:\n        chunks = []\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n            \n            tree = ast.parse(content)\n            rel_path = os.path.relpath(filepath, self.root_dir)\n            filename = os.path.basename(filepath)\n\n            for node in ast.walk(tree):\n                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                    # Skip functions with \"admin\" in the name\n                    if \"admin\" in node.name.lower():\n                        continue\n\n                    # Extract function/method\n                    start_line = node.lineno\n                    end_line = node.end_lineno\n                    code_segment = \"\\n\".join(content.splitlines()[start_line-1:end_line])\n                    \n                    # Generate AI description\n                    # Check cache first\n                    cache_key = f\"{filename}:{node.name}\"\n                    if cache_key in self.cached_descriptions and len(self.cached_descriptions[cache_key].split()) > 5:\n                        ai_description = self.cached_descriptions[cache_key]\n                        # print(f\"Using cached description for {node.name}\")\n                    else:\n                        ai_description = self.generate_description(code_segment, node.name)\n                    \n                    chunk = {\n                        \"type\": \"function\",\n                        \"name\": node.name,\n                        \"filepath\": rel_path,\n                        \"filename\": filename,\n                        \"content\": code_segment,\n                        \"start_line\": start_line,\n                        \"end_line\": end_line,\n                        \"description\": ai_description\n                    }\n                    chunks.append(chunk)\n                elif isinstance(node, ast.ClassDef):\n                     # We might want to chunk the class definition itself (docstring + signature)\n                     # but usually methods are more useful. \n                     # Let's add a chunk for the class docstring/signature if needed.\n                     # For now, focusing on methods/functions as requested.\n                     pass\n                     \n        except Exception as e:\n            print(f\"Error parsing {filepath}: {e}\")\n            # Fallback or skip? Skip for now.\n        \n        return chunks",
    "start_line": 189,
    "end_line": 241,
    "description": "The `chunk_file` function processes a Python file to extract and describe its functions. It parses the file's abstract syntax tree (AST) to identify function definitions.  For each function (excluding those with \"admin\" in the name), it extracts the code segment, its name, and line numbers. It then either retrieves a pre-generated AI description from a cache or uses a `generate_description` function to create a description for the function based on its code. The function returns a list of dictionaries, where each dictionary represents a code chunk with function details and its associated description."
  },
  {
    "type": "function",
    "name": "walk_and_chunk",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def walk_and_chunk(self) -> List[Dict[str, Any]]:\n        all_chunks = []\n        for root, dirs, files in os.walk(self.root_dir):\n            # Modify dirs in-place to skip ignored directories\n            dirs[:] = [d for d in dirs if not self._is_ignored(os.path.join(root, d))]\n            \n            for file in files:\n                filepath = os.path.join(root, file)\n                if self._is_ignored(filepath):\n                    continue\n                if not file.endswith(\".py\"): # Only chunk python files for AST\n                    continue\n                \n                chunks = self.chunk_file(filepath)\n                all_chunks.extend(chunks)\n        return all_chunks",
    "start_line": 243,
    "end_line": 258,
    "description": "The `walk_and_chunk` function recursively traverses a directory, `self.root_dir`, and processes Python files. It uses `os.walk` to list files, skipping those ignored by `_is_ignored()`.  For each Python file, it calls `self.chunk_file` to generate \"chunks\" (likely code segments based on AST analysis). It modifies the `dirs` list in-place to exclude ignored directories. The function returns a list of all collected chunks, represented as dictionaries."
  },
  {
    "type": "function",
    "name": "__init__",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def __init__(self, root_dir: str = PROJECT_ROOT, api_key: str = None):\n        self.root_dir = root_dir\n        self.api_key = api_key\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n        self.chunks = []\n        self.embeddings = None  # Description embeddings (HyDE)\n        self.code_embeddings = None  # Code content embeddings\n        self.keyword_index = None",
    "start_line": 261,
    "end_line": 269,
    "description": "The `__init__` function initializes an object likely designed for semantic search or information retrieval. It loads a SentenceTransformer model (`all-MiniLM-L6-v2`) for generating sentence embeddings and a CrossEncoder model (`cross-encoder/ms-marco-MiniLM-L-6-v2`) for reranking results.  It also initializes instance variables to store text chunks (`self.chunks`), their embeddings (`self.embeddings`), and potentially a keyword index (`self.keyword_index`).  These initialized attributes are crucial for processing text and retrieving relevant information."
  },
  {
    "type": "function",
    "name": "build_index",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def build_index(self):\n        chunker = CodebaseChunker(self.root_dir, api_key=self.api_key)\n        self.chunks = chunker.walk_and_chunk()\n        \n        if not self.chunks:\n            print(\"No chunks found.\")\n            return\n\n        # Embed descriptions (HyDE mode)\n        texts = [c['description'] for c in self.chunks]\n        print(f\"Generating description embeddings for {len(texts)} chunks...\")\n        self.embeddings = self.model.encode(texts, show_progress_bar=True)\n        \n        # Embed code content (Code mode)\n        # Include metadata to help with context\n        code_texts = [f\"File: {c['filename']}\\nFunction: {c['name']}\\n{c['content']}\" for c in self.chunks]\n        print(f\"Generating code embeddings for {len(code_texts)} chunks...\")\n        self.code_embeddings = self.model.encode(code_texts, show_progress_bar=True)\n        \n        # Build keyword index\n        print(\"Building keyword index...\")\n        # Convert chunks to documents format for InvertedIndex\n        docs_for_index = []\n        for idx, chunk in enumerate(self.chunks):\n            docs_for_index.append({\n                'id': idx,\n                'title': chunk['name'],\n                'description': chunk['description']\n            })\n        \n        self.keyword_index = InvertedIndex()\n        self.keyword_index = InvertedIndex()\n        self.keyword_index.build_from_documents(docs_for_index)\n        \n        self.keyword_index.build_from_documents(docs_for_index)\n        \n        print(f\"DEBUG: docs_for_index size: {len(docs_for_index)}\")\n        print(f\"DEBUG: keyword_index.docmap size: {len(self.keyword_index.docmap)}\")\n        \n        self.save_index()",
    "start_line": 271,
    "end_line": 310,
    "description": "The `build_index` function processes a codebase to create search indexes. It uses a `CodebaseChunker` to extract code chunks, then generates embeddings for chunk descriptions using a pre-trained model. It then constructs a keyword index (`InvertedIndex`) from the chunk data, enabling keyword-based search. Finally, the function saves the generated index.  Key parameters include the codebase root (implicit in `CodebaseChunker`) and a pre-trained embedding model (`self.model`). It returns nothing directly but populates internal data structures, including `self.chunks`, `self.embeddings`, and `self.keyword_index`."
  },
  {
    "type": "function",
    "name": "save_index",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def save_index(self):\n        os.makedirs(CACHE_DIR, exist_ok=True)\n        with open(CODEBASE_INDEX_PATH, \"wb\") as f:\n            pickle.dump(self.chunks, f)\n        np.save(CODEBASE_EMBEDDINGS_PATH, self.embeddings)\n        np.save(CODEBASE_EMBEDDINGS_CODE_PATH, self.code_embeddings)\n        \n        # Save keyword index\n        if self.keyword_index:\n            with open(CODEBASE_KEYWORD_INDEX_PATH, \"wb\") as f:\n                pickle.dump(self.keyword_index, f)\n        \n        # Also save as JSON for transparency\n        json_path = os.path.join(CACHE_DIR, \"codebase_data.json\")\n        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.chunks, f, indent=2)\n            \n        print(f\"Index saved to {CODEBASE_INDEX_PATH}\")\n        print(f\"Metadata saved to {json_path}\")",
    "start_line": 312,
    "end_line": 330,
    "description": "The `save_index` function persists code analysis data to disk. It saves the codebase's indexed chunks ( `self.chunks`) and their corresponding embeddings (`self.embeddings`) using `pickle` and `numpy` respectively.  If a keyword index exists (`self.keyword_index`), it's also pickled.  Furthermore, it saves the `self.chunks` as a human-readable JSON file for data transparency. The function utilizes `os.makedirs` to create the cache directory, handles file operations, and prints confirmation messages upon completion."
  },
  {
    "type": "function",
    "name": "load_index",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def load_index(self):\n        if not os.path.exists(CODEBASE_INDEX_PATH) or not os.path.exists(CODEBASE_EMBEDDINGS_PATH):\n            print(\"Index not found. Building new index...\")\n            self.build_index()\n            return\n\n        with open(CODEBASE_INDEX_PATH, \"rb\") as f:\n            self.chunks = pickle.load(f)\n        self.embeddings = np.load(CODEBASE_EMBEDDINGS_PATH)\n        \n        if os.path.exists(CODEBASE_EMBEDDINGS_CODE_PATH):\n            self.code_embeddings = np.load(CODEBASE_EMBEDDINGS_CODE_PATH)\n        else:\n            self.code_embeddings = None\n            print(\"Warning: Code embeddings not found. Code search mode will not work until rebuild.\")\n        \n        # Load keyword index\n        if os.path.exists(CODEBASE_KEYWORD_INDEX_PATH):\n            with open(CODEBASE_KEYWORD_INDEX_PATH, \"rb\") as f:\n                self.keyword_index = pickle.load(f)\n        else:\n            print(\"Keyword index not found. Building keyword index from loaded chunks...\")\n            # Build keyword index from existing chunks without full rebuild\n            docs_for_index = []\n            for idx, chunk in enumerate(self.chunks):\n                docs_for_index.append({\n                    'id': idx,\n                    'title': chunk['name'],\n                    'description': chunk['description']\n                })\n            \n            \n            self.keyword_index = InvertedIndex()\n            self.keyword_index.build_from_documents(docs_for_index)\n            \n            # Save just the keyword index\n            with open(CODEBASE_KEYWORD_INDEX_PATH, \"wb\") as f:\n                pickle.dump(self.keyword_index, f)",
    "start_line": 332,
    "end_line": 369,
    "description": "The `load_index` function loads pre-built code index data. It first checks if the index and embeddings files exist. If not, it triggers a rebuild using `self.build_index()`. Otherwise, it loads the indexed code chunks and embeddings from disk using `pickle` and `numpy`. Crucially, it also handles loading or rebuilding a keyword index. If the keyword index file exists, it's loaded.  If missing, the function constructs a keyword index from the loaded code chunks and saves it for future use, optimizing for faster lookups."
  },
  {
    "type": "function",
    "name": "search",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def search(self, query: str, limit: int = 10, score_threshold: float = 0.01, use_reranking: bool = False, mode: str = \"hyde\") -> List[Dict[str, Any]]:\n        \"\"\"\n        Hybrid search using RRF fusion of semantic and keyword search.\n        mode: \"hyde\" (default, uses description embeddings) or \"code\" (uses code content embeddings)\n        \"\"\"\n        if self.embeddings is None or self.keyword_index is None:\n            self.load_index()\n        \n        # Semantic search\n        query_embedding = self.model.encode(query)\n        \n        # Select embeddings based on mode\n        if mode == \"code\" and self.code_embeddings is not None:\n            target_embeddings = self.code_embeddings\n        else:\n            target_embeddings = self.embeddings\n            \n        semantic_scores = np.dot(target_embeddings, query_embedding) / (\n            np.linalg.norm(target_embeddings, axis=1) * np.linalg.norm(query_embedding)\n        )\n        semantic_indices = np.argsort(semantic_scores)[::-1]\n        \n        # 2. Keyword search (BM25)\n        keyword_results = self.keyword_index.bm25_search(query, limit=limit*3)\n        \n        # 3. RRF Fusion\n        rrf_scores = defaultdict(float)\n        k = DEFAULT_K_VALUE\n        \n        # Add semantic scores\n        for rank, idx in enumerate(semantic_indices):\n            rrf_scores[idx] += rrf_score(rank, k)\n        \n        # Add keyword scores\n        for rank, result in enumerate(keyword_results):\n            idx = result['id']\n            rrf_scores[idx] += rrf_score(rank, k)\n        \n        # Get candidates based on RRF score threshold\n        candidate_count = limit * 3 if use_reranking else limit\n        top_indices = heapq.nlargest(candidate_count, rrf_scores, key=rrf_scores.get)\n        \n        # Filter by score threshold\n        top_indices = [idx for idx in top_indices if rrf_scores[idx] >= score_threshold]\n        \n        if not top_indices:\n            return []\n        \n        # Build initial results\n        results = []\n        for idx in top_indices:\n            if idx < 0 or idx >= len(self.chunks):\n                print(f\"Warning: Index {idx} out of bounds for chunks list (len={len(self.chunks)}). Skipping.\")\n                continue\n                \n            chunk = self.chunks[idx].copy()\n            chunk[\"rrf_score\"] = float(rrf_scores[idx])\n            chunk[\"semantic_score\"] = float(semantic_scores[idx])\n            # Default score to RRF score for compatibility\n            chunk[\"score\"] = chunk[\"rrf_score\"]\n            # print(f\"DEBUG: Assigned score {chunk['score']} to {chunk['name']}\")\n            results.append(chunk)\n        \n        # Apply re-ranking if requested\n        if use_reranking and len(results) > 0:\n            results = self.rerank(query, results, limit)\n        else:\n            results = results[:limit]\n            \n        return results",
    "start_line": 371,
    "end_line": 440,
    "description": "The `search` function performs a hybrid search, combining semantic and keyword search techniques. It takes a `query` string and optional parameters for `limit`, `score_threshold`, and `use_reranking`. It first conducts semantic search using embeddings and cosine similarity, then performs keyword search using BM25.  These results are fused using Reciprocal Rank Fusion (RRF).  Finally, it filters results based on a score threshold and optionally applies re-ranking before returning a list of dictionaries, each containing a chunk's data along with RRF and semantic scores, and the final score."
  },
  {
    "type": "function",
    "name": "rerank",
    "filepath": "cli/lib/codebase_rag.py",
    "filename": "codebase_rag.py",
    "content": "    def rerank(self, query: str, results: List[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:\n        \"\"\"Re-rank results using a cross-encoder model.\"\"\"\n        if not results:\n            return results\n        \n        # Prepare pairs for cross-encoder\n        pairs = [[query, f\"{r['description']}\\n{r['content']}\"] for r in results]\n        \n        # Get cross-encoder scores\n        rerank_scores = self.reranker.predict(pairs)\n        \n        # Sort by rerank scores\n        for i, result in enumerate(results):\n            result[\"rerank_score\"] = float(rerank_scores[i])\n            # Update main score to rerank score\n            result[\"score\"] = result[\"rerank_score\"]\n        \n        results.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n        \n        return results[:limit]",
    "start_line": 442,
    "end_line": 461,
    "description": "The `rerank` function refines search results using a cross-encoder model. It takes a `query`, a list of `results` (dictionaries containing descriptions and content), and a `limit` as input. It forms pairs of the query and result content, then uses the `reranker` model to generate scores for each pair.  It then adds the \"rerank_score\" to the result dictionaries, uses it to set the overall score and sorts the results by these scores, returning the top `limit` results."
  },
  {
    "type": "function",
    "name": "rerank_individual",
    "filepath": "cli/lib/reranking.py",
    "filename": "reranking.py",
    "content": "def rerank_individual(query: str, results: list[dict],limit: int) -> None:\n    for doc in results:\n        prompt = f\"\"\"Rate how well this movie matches the search query.\n\n        Query: \"{query}\"\n        Movie: {doc.get(\"title\", \"\")} - {doc.get(\"document\", \"\")}\n\n        Consider:\n        - Direct relevance to query\n        - User intent (what they're looking for)\n        - Content appropriateness\n\n        Rate 0-10 (10 = perfect match).\n        Give me ONLY the number in your response, no other text or explanation.\n\n        Score:\"\"\"\n\n        response = client.models.generate_content(model=model, contents=prompt)\n        doc['score'] = int((response.text or \"\").strip().strip('\"'))\n    \n    sorted_results = sorted(results, key =lambda x:x['score'], reverse=True)[:limit] \n\n    # logging.info(f\"Cross-encoder Reranking Results: {sorted_results}\")\n    for i, res in enumerate(sorted_results, 1):\n        print(f\"{i}.\\t{res['title']} \\n\\tRRF Score: {res['rrf_score']:.3f} \\n\\tBM25 Rank: {res['bm25_rank']}, Semantic Rank: {res['semantic_rank']}\\n\\t{res['description'][:100]}...\\n\")",
    "start_line": 19,
    "end_line": 43,
    "description": "The `rerank_individual` function refines search results for a given query. It uses a Large Language Model (LLM) to assess the relevance of each document in the `results` list. For each document, a prompt is generated asking the LLM to assign a relevance score (0-10) based on the query and document content. The function then sorts the documents based on these LLM-generated scores and truncates the results to the specified `limit`. Finally, it prints the reranked results, including title, RRF score, BM25 rank, semantic rank, and description."
  },
  {
    "type": "function",
    "name": "rerank_batch",
    "filepath": "cli/lib/reranking.py",
    "filename": "reranking.py",
    "content": "def rerank_batch(query: str, results: list[dict],limit: int) -> None:\n\n    prompt = f\"\"\"Rank these movies by relevance to the search query.\n\n    Query: \"{query}\"\n\n    Movies:\n    {results}\n\n    Return ONLY the IDs in order of relevance (best match first). Return a valid JSON list, nothing     else. Don't put it inside a json markdown. For example, the output should be like :\n\n    [75, 12, 34, 2, 1]\n    \"\"\"\n\n    json_response = client.models.generate_content(model=model, contents=prompt)\n    batch_results = json.loads(json_response.text)\n    docs = []\n    for result in batch_results[:limit]:\n        for doc in results:\n            if doc['id'] == result:\n                docs.append(doc)\n\n    # logging.info(f\"Cross-encoder Reranking Results: {docs}\")\n\n    for i, res in enumerate(docs, 1):\n        print(f\"{i}.\\t{res['title']} \\n\\tRRF Score: {res['rrf_score']:.3f} \\n\\tBM25 Rank: {res['bm25_rank']}, Semantic Rank: {res['semantic_rank']}\\n\\t{res['description'][:100]}...\\n\")",
    "start_line": 45,
    "end_line": 70,
    "description": "The `rerank_batch` function refines search results based on a query using a large language model (LLM). It takes a `query`, a list of `results` (dictionaries with movie data), and a `limit` as input.  The function constructs a prompt for the LLM to rerank the movie IDs by relevance to the query. It parses the LLM's JSON response, reorders the original results based on the reordered IDs, and prints the top results with their ranking information. The function leverages an LLM to perform cross-encoding and improve the ranking of search results."
  },
  {
    "type": "function",
    "name": "rerank_cross_encoder",
    "filepath": "cli/lib/reranking.py",
    "filename": "reranking.py",
    "content": "def rerank_cross_encoder(query: str, results: list[dict],limit: int) -> None:\n    \n    pairs = []\n    for doc in results:\n        text = f\"{doc.get('title', '')} - {doc.get('document', '')}\"\n        pairs.append([query, text])\n    \n    if not pairs:\n        print(\"No results to rerank.\")\n        return\n\n    scores = cross_encoder.predict(pairs)\n    scored_results = list(zip(scores, results))    \n        \n    sorted_scored_results = sorted(scored_results, key=lambda x: x[0], reverse=True)\n\n    docs = []\n    for score, result in sorted_scored_results[:limit]:\n        result['cross-encoder-score'] = score\n        docs.append(result)\n    \n    # logging.info(f\"Cross-encoder Reranking Results: {docs}\")\n\n    for i, res in enumerate(docs, 1):\n        print(f\"{i}.\\t{res['title']} \\n\\tCross Encoder Score: {res['cross-encoder-score']}\\n\\tRRF Score: {res['rrf_score']:.3f} \\n\\tBM25 Rank: {res['bm25_rank']}, Semantic Rank: {res['semantic_rank']}\\n\\t{res['description'][:100]}...\\n\")",
    "start_line": 72,
    "end_line": 96,
    "description": "The `rerank_cross_encoder` function refines search results using a cross-encoder model. It takes a `query` string, a list of initial `results` (dictionaries likely containing document info), and a `limit` to restrict the final output. The function constructs pairs of query-document text, predicts relevance scores using `cross_encoder.predict()`, and then sorts the results based on these scores. It returns a ranked list of the top `limit` documents, each annotated with its cross-encoder score, and prints the ranked documents to the console."
  },
  {
    "type": "function",
    "name": "format_results",
    "filepath": "cli/lib/reranking.py",
    "filename": "reranking.py",
    "content": "def format_results(results: list[dict]) -> str:\n    formatted: str = ''\n    for res in results:\n        formatted += f\"Title: {res['title']} Description: {res['description']}\\n\"\n    return formatted",
    "start_line": 98,
    "end_line": 102,
    "description": "The `format_results` function takes a list of dictionaries, where each dictionary represents a result and contains 'title' and 'description' keys. It iterates through the input list. For each result, it constructs a formatted string combining the title and description. It concatenates the formatted string for each result, separated by a newline character. The function returns a single, multi-line string containing the formatted output of all results."
  },
  {
    "type": "function",
    "name": "evaluate_results",
    "filepath": "cli/lib/reranking.py",
    "filename": "reranking.py",
    "content": "def evaluate_results(query: str, results: list[dict]):\n\n    formatted_results = format_results(results)\n    \n    prompt = f\"\"\"Rate how relevant each result is to this query on a 0-3 scale:\n    \n    Query: \"{query}\"\n\n    \n    Results: \n    \n    {chr(10).join(formatted_results)}\n\n    \n    Scale:\n    \n    - 3: Highly relevant\n    \n    - 2: Relevant\n    \n    - 1: Marginally relevant\n    \n    - 0: Not relevant\n\n    \n    Do NOT give any numbers out of 0, 1, 2 or 3.\n\n    \n    Return ONLY the scores in teh same order you were given the documents. Return a valid JSON\n    list, nothing else. Don't use markdown in your response. For example: \n    \n    [2, 0, 3, 2, 0, 1]\n    \n    \"\"\"\n\n    response = client.models.generate_content(model=model, contents=prompt)\n\n    res_list = json.loads(response.text)\n    for i in range(0, len(res_list)):\n        print(f\"{results[i]['title']} : {res_list[i]}/3\\n\")",
    "start_line": 104,
    "end_line": 143,
    "description": "The `evaluate_results` function assesses the relevance of search results to a given query.  It takes a query string and a list of result dictionaries as input. It formats the results using `format_results` and constructs a prompt for an LLM (likely using a client like OpenAI's) to rate each result's relevance on a 0-3 scale. The LLM's JSON-formatted response, containing the relevance scores, is then parsed, and each result's title is printed along with its assigned score."
  },
  {
    "type": "function",
    "name": "re_rank",
    "filepath": "cli/lib/reranking.py",
    "filename": "reranking.py",
    "content": "def re_rank(query: str, results: list[dict], limit: int ,method: Optional[str] = None) -> str:\n    match method:\n        case \"individual\":\n            return rerank_individual(query, results, limit)\n        case \"batch\":\n            return rerank_batch(query, results, limit)\n        case \"cross_encoder\":\n            return rerank_cross_encoder(query, results, limit)\n        case _:\n            return",
    "start_line": 145,
    "end_line": 154,
    "description": "The `re_rank` function reorders a list of search results based on a given query. It accepts a `query` string, a `results` list of dictionaries, and a `limit` integer determining the maximum number of results to return. The core logic utilizes a `method` parameter (optional string), which dictates the re-ranking algorithm: \"individual,\" \"batch,\" or \"cross_encoder.\" The function then dispatches the re-ranking task to the corresponding internal function, returning the re-ranked results based on the chosen method. If no method is provided or an unsupported one is passed, it returns None."
  },
  {
    "type": "function",
    "name": "verify_image_embedding",
    "filepath": "cli/lib/multimodal_search.py",
    "filename": "multimodal_search.py",
    "content": "def verify_image_embedding(image: str):\n    image_path = os.path.join(PROJECT_ROOT, image)\n    multimodal_search = MultiModalSearch()\n    embedding = multimodal_search.embed_image(image_path)\n    print(f\"Embedding shape: {embedding.shape[0]} dimensions\")",
    "start_line": 49,
    "end_line": 53,
    "description": "The `verify_image_embedding` function generates and validates an image embedding. It takes an image file path (relative to `PROJECT_ROOT`) as input.  It utilizes a `MultiModalSearch` object to create an embedding of the image. Finally, it prints the dimensionality (number of features) of the generated embedding. This serves as a basic check to confirm that the image embedding process completed successfully and that a vector representation of the image was created."
  },
  {
    "type": "function",
    "name": "image_search_command",
    "filepath": "cli/lib/multimodal_search.py",
    "filename": "multimodal_search.py",
    "content": "def image_search_command(image: str):\n    documents = load_movies()\n    image_path = os.path.join(PROJECT_ROOT, image)\n    multimodal_search = MultiModalSearch(documents)\n    multimodal_search.load_or_create_embeddings()\n    results = multimodal_search.search_with_image(image_path)\n    for i, res in enumerate(results, 1):\n        print(f\"\\n{i}. {res['title']} (similarity: {res['score']:.3f})\\n   {res['description'][:100]}\")",
    "start_line": 55,
    "end_line": 62,
    "description": "The `image_search_command` function searches for movie descriptions similar to a provided image. It takes an image filename as input. It utilizes a `MultiModalSearch` object, which likely combines text and image-based search. The function loads movie data, computes/loads embeddings, and then performs a search using the provided image.  It returns a ranked list of movie titles, scores (similarity), and descriptions, which are then printed to the console."
  },
  {
    "type": "function",
    "name": "__init__",
    "filepath": "cli/lib/multimodal_search.py",
    "filename": "multimodal_search.py",
    "content": "    def __init__(self,documents,  model_name=\"clip-ViT-B-32\"):\n        self.model = SentenceTransformer(model_name)\n        self.documents = documents\n        self.texts = [f\"{doc['title']}: {doc['description']}\" for doc in documents]\n        self.clip_embeddings_path = os.path.join(CACHE_DIR, \"clip_embeddings.npy\")\n        self.text_embeddings = None",
    "start_line": 10,
    "end_line": 15,
    "description": "The `__init__` function initializes a class designed for document embedding and search, likely using a CLIP model. It takes a list of document dictionaries as input. It loads a SentenceTransformer model (defaults to clip-ViT-B-32) and stores the provided documents. Crucially, it creates a list of concatenated text strings combining document titles and descriptions. Finally, it sets up a file path for storing clip embeddings, although the embeddings themselves are not computed in this function. The function's main function is to setup the class for further processing of text embeddings."
  },
  {
    "type": "function",
    "name": "build_embeddings",
    "filepath": "cli/lib/multimodal_search.py",
    "filename": "multimodal_search.py",
    "content": "    def build_embeddings(self) -> None:\n        self.text_embeddings = self.model.encode(self.texts, show_progress_bar=True)\n        with open(self.clip_embeddings_path, 'wb') as f:\n            np.save(f, self.text_embeddings)",
    "start_line": 17,
    "end_line": 20,
    "description": "The `build_embeddings` function processes text data to generate embeddings and save them to a file.  It utilizes a pre-trained model (`self.model`) to encode a list of text strings (`self.texts`) into numerical representations (`self.text_embeddings`). A progress bar is displayed during the encoding process. The generated embeddings are then saved as a NumPy array to the specified file path (`self.clip_embeddings_path`) using the `.np.save` method, allowing for persistent storage of the encoded text."
  },
  {
    "type": "function",
    "name": "load_or_create_embeddings",
    "filepath": "cli/lib/multimodal_search.py",
    "filename": "multimodal_search.py",
    "content": "    def load_or_create_embeddings(self) -> None:\n        if not os.path.exists(self.clip_embeddings_path):\n            return self.build_embeddings()\n        with open(self.clip_embeddings_path, \"rb\") as f:\n            self.text_embeddings = np.load(f)",
    "start_line": 22,
    "end_line": 26,
    "description": "The `load_or_create_embeddings` function manages text embeddings.  It checks for the existence of an existing embedding file specified by `self.clip_embeddings_path`. If the file exists, the function loads the embeddings from the file into the `self.text_embeddings` attribute using NumPy.  Otherwise, it calls `self.build_embeddings()` to create and store the embeddings. The function takes no parameters and returns nothing, modifying the state of the object."
  },
  {
    "type": "function",
    "name": "embed_image",
    "filepath": "cli/lib/multimodal_search.py",
    "filename": "multimodal_search.py",
    "content": "    def embed_image(self, image_path: str):\n        if image_path.isspace() or len(image_path) == 0:\n            raise ValueError(\"Empty string\")\n        img = Image.open(image_path)\n        embedding = self.model.encode([img])\n        return embedding[0]",
    "start_line": 28,
    "end_line": 33,
    "description": "The `embed_image` function processes a provided image and generates an embedding vector. It takes the `image_path` (a string) as input.  First, it validates that the path is not empty or whitespace.  Then, it opens the image using the `Image` library. The core functionality involves using a pre-trained `self.model` to encode the image into a numerical embedding. Finally, it returns the first embedding vector, a numerical representation of the input image."
  },
  {
    "type": "function",
    "name": "search_with_image",
    "filepath": "cli/lib/multimodal_search.py",
    "filename": "multimodal_search.py",
    "content": "    def search_with_image(self, image_path: str):\n        img_embedding = self.embed_image(image_path)\n        similarity_list = []\n        for text_embedding, doc in zip(self.text_embeddings, self.documents):\n            similarity = float(cosine_similarity(img_embedding, text_embedding))\n            similarity_list.append({\n                \"title\" : doc['title'],\n                \"description\" : doc['description'],\n                \"score\" : similarity\n            })\n        \n        most_similar = heapq.nlargest(5, similarity_list, key=lambda item: item['score'])\n        return most_similar",
    "start_line": 35,
    "end_line": 47,
    "description": "The `search_with_image` function performs an image-based search within a pre-indexed document collection. It takes the `image_path` as input, embeds the image using `embed_image`, and calculates cosine similarity between the image embedding and existing text embeddings associated with documents. The function returns the top 5 most similar documents, each represented as a dictionary including the title, description, and similarity score. This uses the cosine similarity algorithm and leverages the `heapq` library to efficiently find the top results."
  },
  {
    "type": "function",
    "name": "_load_from_streamlit_secrets",
    "filepath": "cli/lib/search_utils.py",
    "filename": "search_utils.py",
    "content": "def _load_from_streamlit_secrets(key: str) -> Optional[str]:\n    try:\n        import streamlit as st\n        from streamlit.errors import StreamlitSecretNotFoundError\n    except ImportError:\n        return None\n\n    try:\n        return st.secrets.get(key)\n    except StreamlitSecretNotFoundError:\n        return None",
    "start_line": 30,
    "end_line": 40,
    "description": "The `_load_from_streamlit_secrets` function retrieves a secret value from Streamlit's secrets management. It takes a `key` (string) as input, representing the secret's name. It attempts to import `streamlit` and access the secret using `st.secrets.get(key)`. If Streamlit isn't installed or the secret isn't found (handled by `StreamlitSecretNotFoundError`), the function returns `None`.  This function provides a way to securely load configuration data within a Streamlit application."
  },
  {
    "type": "function",
    "name": "get_gemini_api_key",
    "filepath": "cli/lib/search_utils.py",
    "filename": "search_utils.py",
    "content": "def get_gemini_api_key() -> str:\n    api_key = os.environ.get(\"GEMINI_API_KEY\")\n    if api_key:\n        return api_key\n\n    api_key = _load_from_streamlit_secrets(\"GEMINI_API_KEY\")\n    if api_key:\n        return api_key\n\n    raise RuntimeError(\"Missing `GEMINI_API_KEY`. Set it via env var or Streamlit secrets.\")",
    "start_line": 43,
    "end_line": 52,
    "description": "The `get_gemini_api_key` function retrieves a Gemini API key. It first attempts to fetch the key from the environment variable `GEMINI_API_KEY`. If not found, it tries to load it from Streamlit secrets using a private helper function.  The function returns the API key as a string if successful.  If the key is not found in either location, it raises a `RuntimeError` indicating the key is missing and provides instructions on how to set it."
  },
  {
    "type": "function",
    "name": "load_movies",
    "filepath": "cli/lib/search_utils.py",
    "filename": "search_utils.py",
    "content": "def load_movies() -> list[dict]:\n    with open(MOVIE_PATH, \"r\") as file:\n        data = json.load(file)\n    return data[\"movies\"]",
    "start_line": 55,
    "end_line": 58,
    "description": "The `load_movies` function retrieves movie data from a JSON file. It opens the file specified by the `MOVIE_PATH` constant in read mode and uses the `json.load()` method to parse the file content.  It expects the JSON to contain a top-level dictionary with a \"movies\" key, which holds a list of movie dictionaries. The function returns this list of movie dictionaries, representing the loaded movie data."
  },
  {
    "type": "function",
    "name": "load_testcases",
    "filepath": "cli/lib/search_utils.py",
    "filename": "search_utils.py",
    "content": "def load_testcases() -> list[dict]:\n    with open(TESTCASES_PATH, \"r\") as file:\n        data = json.load(file)\n    return data[\"test_cases\"]",
    "start_line": 61,
    "end_line": 64,
    "description": "The `load_testcases` function retrieves test case data from a JSON file. It reads the contents of the file specified by `TESTCASES_PATH`, which is assumed to be a pre-defined constant.  The function uses the `json` library to parse the file's content.  Crucially, it accesses the \"test_cases\" key within the loaded JSON data. The function returns a list of dictionaries, where each dictionary represents a single test case."
  },
  {
    "type": "function",
    "name": "read_stopwords",
    "filepath": "cli/lib/search_utils.py",
    "filename": "search_utils.py",
    "content": "def read_stopwords() -> list[str]:\n    with open(STOPWORDS_PATH, \"r\") as file:\n        data = file.read().splitlines()\n    return data",
    "start_line": 67,
    "end_line": 70,
    "description": "The `read_stopwords` function reads a list of stop words from a file specified by the `STOPWORDS_PATH` constant. It opens the file in read mode, reads all lines, and then splits the content into a list of strings, with each string representing a stop word. The function returns this list of stop words. The core logic involves reading the file and utilizing the `splitlines()` method to separate words based on newline characters."
  },
  {
    "type": "function",
    "name": "search_chunked_command",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "def search_chunked_command(query: str, limit: int = DEFAULT_SEARCH_LIMIT) -> list[dict]:\n    documents = load_movies()\n    chunked_ss = ChunkedSemanticSearch()\n    chunk_embeddings = chunked_ss.load_or_create_chunk_embeddings(documents)\n    results = chunked_ss.search_chunks(query, limit)\n    return results",
    "start_line": 156,
    "end_line": 161,
    "description": "The `search_chunked_command` function searches a movie database using a chunked semantic search approach. It loads movies, computes chunk embeddings (or loads existing ones), and then performs a semantic search based on the provided `query`.  The search utilizes a `ChunkedSemanticSearch` class to handle the chunking and embedding logic.  It returns a list of dictionaries (`results`), representing the search hits, limited by the optional `limit` parameter (defaults to `DEFAULT_SEARCH_LIMIT`)."
  },
  {
    "type": "function",
    "name": "embed_chunks_command",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "def embed_chunks_command():\n    chunked_semantic_search = ChunkedSemanticSearch()\n    documents = load_movies()\n    embeddings = chunked_semantic_search.load_or_create_chunk_embeddings(documents)\n    print(f\"Generated {len(embeddings)} chunked embeddings\")",
    "start_line": 163,
    "end_line": 167,
    "description": "The `embed_chunks_command` function generates and stores embeddings for movie data. It utilizes a `ChunkedSemanticSearch` object to process the data. Key actions include loading movie documents using `load_movies()`, and then generating chunk embeddings with `load_or_create_chunk_embeddings()`. The function returns no explicit value but prints the number of generated chunk embeddings to the console, providing a status update. This process likely involves chunking the documents and creating semantic representations (embeddings) for efficient search and retrieval."
  },
  {
    "type": "function",
    "name": "search_command",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "def search_command(query: str, limit:int = DEFAULT_SEARCH_LIMIT):\n    semantic_search = SemanticSearch()\n    documents = load_movies()\n    embeddings = semantic_search.load_or_create_embeddings(documents)\n    results = semantic_search.search(query, limit)\n    for i, res in enumerate(results, 1):\n        print(f\"{i}. {res['title']} (score: {res['score']:.4f})\")",
    "start_line": 170,
    "end_line": 176,
    "description": "The `search_command` function performs a semantic search for movies based on a user-provided `query`. It utilizes a `SemanticSearch` class to load movie data, generate embeddings, and execute the search.  The function takes a search `query` (string) and an optional `limit` (integer, defaults to `DEFAULT_SEARCH_LIMIT`) to restrict the number of results. It returns nothing directly, but it prints the search results, including movie titles and relevance scores, to the console, formatted with numbering."
  },
  {
    "type": "function",
    "name": "chunk_command",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "def chunk_command(query: str, chunk_size: int = DEFAULT_CHUNK_SIZE, overlap: int = DEFAULT_OVERLAP_SIZE) -> list[str]:\n    query_split = query.split(\" \")\n    results = []\n    for i in range(0, len(query_split)-overlap, chunk_size):  \n        if i == 0:\n            results.append(\" \".join(query_split[i:i+chunk_size]))  \n            continue\n        results.append(\" \".join(query_split[i-overlap:i+chunk_size-overlap])) \n    return results",
    "start_line": 178,
    "end_line": 186,
    "description": "The `chunk_command` function splits a given query string into overlapping chunks. It takes the query as a string, along with optional chunk size and overlap parameters (defaulting to `DEFAULT_CHUNK_SIZE` and `DEFAULT_OVERLAP_SIZE`). The function splits the query into words, then iterates through the word list, constructing chunks with the specified overlap. It returns a list of strings, where each string represents a chunk of the original query. The first chunk starts at the beginning, while subsequent chunks overlap with previous ones."
  },
  {
    "type": "function",
    "name": "semantic_chunk_command",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "def semantic_chunk_command(input: str, max_chunk_size: int = DEFAULT_MAX_CHUNK_SIZE, overlap: int = DEFAULT_OVERLAP_SIZE) -> list[str]:\n    input = input.strip()\n    if len(input) == 0:\n        return []\n    input_split = re.split(r'(?<=[.!?])\\s+', input)\n    results = []\n    for i in range(0, len(input_split) - overlap, max_chunk_size-overlap):  \n        results.append(\" \".join(input_split[i:i+max_chunk_size])) \n    return results",
    "start_line": 188,
    "end_line": 196,
    "description": "The `semantic_chunk_command` function segments a string into semantically meaningful chunks. It takes an input string and optionally a `max_chunk_size` and `overlap` value.  The input is first split into sentences using sentence-ending punctuation.  Then, it iterates through the sentences, joining them into chunks of at most `max_chunk_size` sentences, with an overlap of `overlap` sentences.  It returns a list of these resulting chunks, allowing for processing of large texts in smaller, more manageable units.  Empty inputs return an empty list."
  },
  {
    "type": "function",
    "name": "embed_query_text",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "def embed_query_text(query):\n    semantic_search = SemanticSearch()\n    embedding = semantic_search.generate_embeddings(query)\n    print(f\"Query: {query}\")\n    print(f\"First 5 dimensions: {embedding[:5]}\")\n    print(f\"Shape: {embedding.shape}\")",
    "start_line": 198,
    "end_line": 203,
    "description": "The `embed_query_text` function takes a text `query` as input and generates a numerical embedding representing its semantic meaning. It utilizes a `SemanticSearch` object to produce the embedding. The function then prints the original query, the first five dimensions of the embedding, and the embedding's shape.  Effectively, this function transforms text into a vector suitable for semantic search tasks, allowing for similarity comparisons between queries and other text data based on their meaning."
  },
  {
    "type": "function",
    "name": "embed_text",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "def embed_text(text):\n    semantic_search = SemanticSearch()\n    embedding = semantic_search.generate_embeddings(text)\n    print(f\"Text: {text}\")\n    print(f\"First 3 dimensions: {embedding[:3]}\")\n    print(f\"Dimensions: {embedding.shape[0]}\")",
    "start_line": 206,
    "end_line": 211,
    "description": "The `embed_text` function transforms input text into a numerical embedding vector using a semantic search model. It takes a `text` string as input and utilizes a `SemanticSearch` object to generate the embedding. It prints the original text and the first three dimensions of the generated embedding, followed by the total number of dimensions (vector length).  The function does not return any values explicitly, but rather provides output for analysis and debugging."
  },
  {
    "type": "function",
    "name": "verify_embeddings",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "def verify_embeddings():\n    semantic_search = SemanticSearch()\n    documents = load_movies()\n    embeddings = semantic_search.load_or_create_embeddings(documents)\n    print(f\"Number of docs:   {len(documents)}\")\n    print(f\"Embeddings shape: {embeddings.shape[0]} vectors in {embeddings.shape[1]} dimensions\")",
    "start_line": 213,
    "end_line": 218,
    "description": "The `verify_embeddings` function validates the creation and shape of text embeddings. It first loads movie documents. Then, it utilizes a `SemanticSearch` object to either load pre-existing embeddings or generate new ones for these documents. The function then prints the total number of processed documents and the dimensions (shape) of the resulting embedding matrix. This shape check ensures the embeddings have been correctly created for downstream semantic search tasks."
  },
  {
    "type": "function",
    "name": "verify_model",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "def verify_model() -> None:\n    semantic_search = SemanticSearch()\n    model = semantic_search.model\n    print(f\"Model loaded: {model}\")\n    print(f\"Max sequence length: {model.max_seq_length}\")",
    "start_line": 220,
    "end_line": 224,
    "description": "The `verify_model` function checks the successful loading and basic properties of a semantic search model. It initializes a `SemanticSearch` object, accesses its underlying model, and prints the model's name and maximum sequence length to the console. This simple verification confirms that the model has been loaded correctly and allows for a quick inspection of its configuration. The function takes no parameters and returns nothing (None)."
  },
  {
    "type": "function",
    "name": "cosine_similarity",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "def cosine_similarity(vec1, vec2):\n    dot_product = np.dot(vec1, vec2)\n    norm1 = np.linalg.norm(vec1)\n    norm2 = np.linalg.norm(vec2)\n\n    if norm1 == 0 or norm2 == 0:\n        return 0.0\n\n    return dot_product / (norm1 * norm2)",
    "start_line": 226,
    "end_line": 234,
    "description": "The `cosine_similarity` function calculates the cosine similarity between two numerical vectors, `vec1` and `vec2`.  It utilizes the dot product and vector norms (Euclidean lengths) to determine the cosine of the angle between the vectors. The function first computes the dot product, followed by the norms of each vector. It returns 0.0 if either vector has a zero norm, preventing division by zero. Otherwise, it returns the cosine similarity score, a value between -1 and 1, indicating the degree of similarity between the vectors."
  },
  {
    "type": "function",
    "name": "__init__",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "    def __init__(self) -> None:\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.embeddings = None\n        self.documents = None\n        self.document_map = {}",
    "start_line": 12,
    "end_line": 16,
    "description": "The `__init__` function initializes a new instance of a class, likely a subclass. It calls the parent class's `__init__` method using `super().__init__()` for inheritance purposes. Subsequently, it sets two instance attributes: `self.chunk_embeddings` and `self.chunk_metadata` to `None`. This function serves to prepare a new object, setting initial state variables that will likely hold data related to chunk embeddings and their associated metadata. The function has no input parameters beyond `self` and returns `None`."
  },
  {
    "type": "function",
    "name": "generate_embeddings",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "    def generate_embeddings(self, text: str):\n        if text.isspace() or len(text) == 0:\n            raise ValueError(\"Empty string\")\n        embedding = self.model.encode([text])\n        return embedding[0]",
    "start_line": 18,
    "end_line": 22,
    "description": "The `generate_embeddings` function converts a given text string into a numerical vector representation (embedding) using a pre-trained language model. It takes a single string, `text`, as input. Before processing, it checks for empty or whitespace-only input and raises a ValueError if encountered. The function then uses the model (`self.model`)'s `encode` method to generate the embedding. It returns the embedding vector for the input text, flattened to a 1D array."
  },
  {
    "type": "function",
    "name": "build_embeddings",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "    def build_embeddings(self, documents: list[dict]):\n        self.documents = documents\n        doc_list = []\n        for doc in documents:\n            self.document_map[doc['id']] = doc\n            doc_list.append(f\"{doc['title']}: {doc['description']}\")\n        self.embeddings = self.model.encode(doc_list,show_progress_bar=True)\n        with open(EMBEDDING_PATH, 'wb') as f:\n            np.save(f, self.embeddings)\n        return self.embeddings",
    "start_line": 24,
    "end_line": 33,
    "description": "The `build_embeddings` function processes a list of document dictionaries, generating embeddings for each. It extracts titles and descriptions to create a document list.  Using a pre-defined language model (`self.model`), it encodes these document strings into numerical embeddings, displaying a progress bar during the process. The function then stores the generated embeddings using NumPy to the specified EMBEDDING_PATH for later use.  The computed embeddings are returned. It also constructs a document map using the document IDs."
  },
  {
    "type": "function",
    "name": "load_or_create_embeddings",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "    def load_or_create_embeddings(self, documents):\n        self.documents = documents\n        for doc in documents:\n            self.document_map[doc['id']] = doc\n        if not os.path.exists(EMBEDDING_PATH):\n            return self.build_embeddings(documents)\n        with open(EMBEDDING_PATH, \"rb\") as f:\n            self.embeddings = np.load(f)\n        return self.embeddings",
    "start_line": 35,
    "end_line": 43,
    "description": "The `load_or_create_embeddings` function manages document embeddings. It first stores the input `documents` and maps them by ID. It then checks if pre-computed embeddings exist at `EMBEDDING_PATH`. If the file exists, it loads the embeddings using `np.load`. Otherwise, it calls `self.build_embeddings(documents)` to generate and store them. The function returns the loaded or generated NumPy array of embeddings, representing the documents' vector representations."
  },
  {
    "type": "function",
    "name": "search",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "    def search(self, query: str, limit:int = DEFAULT_SEARCH_LIMIT) -> list[dict]:\n        if len(self.embeddings) == 0 or self.embeddings is None:\n            raise ValueError(\"No embeddings loaded. Call `load_or_create_embeddings` first.\")\n        query_embedding = self.generate_embeddings(query)\n        score_list = []\n        for i in range(len(self.documents)):\n            similarity_score = cosine_similarity(query_embedding, self.embeddings[i])\n            document = self.documents[i].copy()\n            del document['id']\n            document['score'] = similarity_score\n            score_list.append(document)\n        sorted_list = sorted(score_list, key=lambda item: item['score'], reverse=True)\n        return sorted_list[:limit]",
    "start_line": 45,
    "end_line": 57,
    "description": "The `search` function performs a semantic search within a document collection. It takes a `query` string and an optional `limit` for results as input.  It first generates an embedding for the query.  Then, it calculates the cosine similarity between the query embedding and pre-computed document embeddings.  It returns a list of dictionaries, each containing a document's content and its similarity score, sorted by score in descending order, truncated to the specified `limit`.  A `ValueError` is raised if no embeddings are loaded."
  },
  {
    "type": "function",
    "name": "__init__",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "    def __init__(self) -> None:\n        super().__init__()\n        self.chunk_embeddings = None\n        self.chunk_metadata = None",
    "start_line": 60,
    "end_line": 63,
    "description": "The `__init__` function initializes a new instance of a class, likely a subclass. It calls the parent class's `__init__` method using `super().__init__()` for inheritance purposes. Subsequently, it sets two instance attributes: `self.chunk_embeddings` and `self.chunk_metadata` to `None`. This function serves to prepare a new object, setting initial state variables that will likely hold data related to chunk embeddings and their associated metadata. The function has no input parameters beyond `self` and returns `None`."
  },
  {
    "type": "function",
    "name": "build_chunk_embeddings",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "    def build_chunk_embeddings(self, documents):\n        self.documents = documents\n        doc_list = []\n\n        chunk_list : list[str]= []\n        chunk_metadata = []\n\n        for doc in documents:\n            self.document_map[doc['id']] = doc\n            doc_list.append(f\"{doc['title']}: {doc['description']}\")\n            \n            if doc['description'].isspace() or len(doc['description']) == 0:\n                continue\n            chunks = semantic_chunk_command(doc['description'],4,1)\n            chunk_len = len(chunks)\n\n            for chunk in chunks:\n                chunk_list.append(chunk)\n                chunk_metadata.append({\n                    'movie_idx' : doc['id'],\n                    'chunk_idx' : chunks.index(chunk),\n                    'total_chunks' : chunk_len\n                })\n\n        self.chunk_embeddings = self.model.encode(chunk_list,show_progress_bar=True)\n        self.chunk_metadata = {\n            \"chunks\": chunk_metadata,\n            \"total_chunks\": len(chunk_list),\n        }\n\n        with open(CHUNK_EMBEDDING_PATH, 'wb') as f:\n            np.save(f, self.chunk_embeddings)\n        with open(CHUNK_METADATA_PATH, 'w') as f:\n            json.dump(self.chunk_metadata, f, indent=2)\n\n        return self.chunk_embeddings",
    "start_line": 65,
    "end_line": 100,
    "description": "The `build_chunk_embeddings` function processes a list of document dictionaries, generating embeddings for text chunks. It first combines each document's title and description. Then, it utilizes `semantic_chunk_command` to divide descriptions into chunks and creates metadata associating each chunk with its original document.  It employs a pre-trained model (accessed via `self.model.encode`) to generate embeddings for these chunks. The function stores the embeddings as a NumPy array and the metadata as a JSON file, both on disk. Finally, it returns the generated chunk embeddings."
  },
  {
    "type": "function",
    "name": "load_or_create_chunk_embeddings",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "    def load_or_create_chunk_embeddings(self, documents):\n        self.documents = documents\n\n        for doc in documents:\n            self.document_map[doc['id']] = doc\n\n        if not os.path.exists(CHUNK_EMBEDDING_PATH) or not os.path.exists(CHUNK_METADATA_PATH):\n            return self.build_chunk_embeddings(documents)\n        \n        with open(CHUNK_EMBEDDING_PATH, \"rb\") as f:\n            self.chunk_embeddings = np.load(f)\n        with open(CHUNK_METADATA_PATH, \"r\") as f:\n            self.chunk_metadata = json.load(f)\n            \n        return self.chunk_embeddings",
    "start_line": 102,
    "end_line": 116,
    "description": "The `load_or_create_chunk_embeddings` function manages document embeddings. It first checks if pre-computed embeddings and metadata exist at predefined file paths. If not, it calls `build_chunk_embeddings` (not shown) to generate them.  If files exist, it loads NumPy embeddings and JSON metadata from disk.  It takes a list of `documents` as input, storing document data internally.  The function returns the loaded or newly generated chunk embeddings (NumPy array)."
  },
  {
    "type": "function",
    "name": "search_chunks",
    "filepath": "cli/lib/semantic_search.py",
    "filename": "semantic_search.py",
    "content": "    def search_chunks(self, query: str, limit: int = DEFAULT_SEARCH_LIMIT):\n        if len(self.chunk_embeddings) == 0:\n            raise ValueError(\"No embeddings loaded. Call `load_or_create_chunk_embeddings` first.\")\n        query_embedding = self.generate_embeddings(query)\n        chunk_score_list = []\n\n        for i in range(len(self.chunk_embeddings)):\n            embedding = self.chunk_embeddings[i]\n            metadata = self.chunk_metadata['chunks'][i]\n            score = cosine_similarity(embedding, query_embedding)\n            chunk_score_list.append({\n                'chunk_idx': metadata['chunk_idx'],\n                'movie_idx': metadata['movie_idx'],\n                'score': score,\n            })\n        \n        movie_to_score_dict = {}\n        for chunk_score in chunk_score_list:\n            movie_index = chunk_score['movie_idx']\n            if movie_index not in movie_to_score_dict or (chunk_score['score']> movie_to_score_dict[movie_index]):\n                movie_to_score_dict[movie_index] = chunk_score['score']\n        \n        movie_scores_sorted = sorted(movie_to_score_dict.items(), key= lambda item : item[1], reverse=True)\n\n        results = []\n        for key, value in movie_scores_sorted:\n            if len(results) == limit:\n                break\n            movie = self.document_map[key]\n            results.append({\n                'id'    : movie['id'],\n                'title' : movie['title'],\n                'description'  : movie['description'][:100],\n                'score' : value\n                })\n        return results",
    "start_line": 118,
    "end_line": 153,
    "description": "The `search_chunks` function searches for relevant movie chunks within a pre-embedded dataset based on a given query string.  It first generates an embedding for the query.  Then, it calculates the cosine similarity between the query embedding and each pre-computed chunk embedding. The function aggregates scores by movie index, selecting the highest score for each movie.  Finally, it returns a limited number (default: `DEFAULT_SEARCH_LIMIT`) of the most relevant movie results, including their IDs, titles, descriptions (truncated), and calculated scores. It requires chunk embeddings to be loaded beforehand."
  }
]